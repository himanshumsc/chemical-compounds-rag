2025-11-23 04:40:18,965 [INFO] ======================================================================
2025-11-23 04:40:18,965 [INFO] QWEN Answer Regeneration with vLLM + RAG
2025-11-23 04:40:18,965 [INFO] ======================================================================
2025-11-23 04:40:18,965 [INFO] Input directory: /home/himanshu/dev/output/qwen_regenerated
2025-11-23 04:40:18,965 [INFO] Output directory: /home/himanshu/dev/output/qwen_rag
2025-11-23 04:40:18,966 [INFO] QA directory: /home/himanshu/dev/test/data/processed/qa_pairs_individual_components
2025-11-23 04:40:18,966 [INFO] Max tokens: 500
2025-11-23 04:40:18,966 [INFO] vLLM available: True
2025-11-23 04:40:18,966 [INFO] ChromaDB available: True
2025-11-23 04:40:18,966 [INFO] ChromaDB path: /home/himanshu/dev/data/chromadb
2025-11-23 04:40:18,966 [INFO] Test limit: 3
2025-11-23 04:40:18,966 [INFO] ======================================================================
2025-11-23 04:40:18,966 [INFO] Initializing model...
2025-11-23 04:40:19,978 [INFO] Initializing vLLM for ALL questions (text and vision)...
2025-11-23 04:40:19,979 [INFO] vLLM is REQUIRED - no Transformers fallback available
2025-11-23 04:40:19,979 [INFO] Environment variables set: VLLM_SKIP_MM_PROFILE=1, SKIP_MM_PROFILE=1
2025-11-23 04:40:19,979 [INFO] Starting vLLM initialization (this may take 20-30 seconds)...
2025-11-23 04:40:44,788 [INFO] vLLM initialized successfully for all question types
2025-11-23 04:40:44,788 [INFO] Initializing ChromaDB search system from /home/himanshu/dev/data/chromadb...
2025-11-23 04:40:48,734 [INFO] ChromaDB search system initialized successfully
2025-11-23 04:40:48,734 [INFO] Model initialized. Using vLLM: True, RAG: True
2025-11-23 04:40:48,735 [INFO] TEST MODE: Processing 3 files
2025-11-23 04:40:48,735 [INFO] Total files to process: 3
2025-11-23 04:40:48,735 [INFO] 
Processing batch 1 (3 files)
2025-11-23 04:40:48,735 [INFO] Loading: 100_Nicotine__answers.json (source: 100_Nicotine.json)
2025-11-23 04:40:48,735 [INFO] Loading: 101_Nitric_Acid__answers.json (source: 101_Nitric_Acid.json)
2025-11-23 04:40:48,736 [INFO] Loading: 102_Nitric_Oxide__answers.json (source: 102_Nitric_Oxide.json)
2025-11-23 04:40:50,093 [INFO]   Batch Q1: Generating 3 vision questions with RAG...
2025-11-23 04:40:55,958 [ERROR]     Q1 batch error: The decoder prompt (length 7041) is longer than the maximum model length of 4096. Make sure that `max_model_len` is no smaller than the number of text tokens plus multimodal tokens. For image inputs, the number of image tokens depends on the number of images, and possibly their aspect ratios as well.
Traceback (most recent call last):
  File "/home/himanshu/dev/code/multimodal_qa_runner_vllm.py", line 605, in regenerate_from_existing_answers
    q1_batch_results = wrapper.generate_with_vision_batch(q1_prompts, q1_images, max_new_tokens=max_new_tokens)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/himanshu/dev/code/multimodal_qa_runner_vllm.py", line 321, in generate_with_vision_batch
    outputs = self.vllm_llm.generate(multimodal_prompts, self.vllm_sampling_params)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/himanshu/dev/code/.venv_phi4_req/lib64/python3.11/site-packages/vllm/entrypoints/llm.py", line 440, in generate
    self._validate_and_add_requests(
  File "/home/himanshu/dev/code/.venv_phi4_req/lib64/python3.11/site-packages/vllm/entrypoints/llm.py", line 1613, in _validate_and_add_requests
    raise e
  File "/home/himanshu/dev/code/.venv_phi4_req/lib64/python3.11/site-packages/vllm/entrypoints/llm.py", line 1601, in _validate_and_add_requests
    request_id = self._add_request(
                 ^^^^^^^^^^^^^^^^^^
  File "/home/himanshu/dev/code/.venv_phi4_req/lib64/python3.11/site-packages/vllm/entrypoints/llm.py", line 1700, in _add_request
    engine_request, tokenization_kwargs = self._process_inputs(
                                          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/himanshu/dev/code/.venv_phi4_req/lib64/python3.11/site-packages/vllm/entrypoints/llm.py", line 1680, in _process_inputs
    engine_request = self.processor.process_inputs(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/himanshu/dev/code/.venv_phi4_req/lib64/python3.11/site-packages/vllm/v1/engine/processor.py", line 442, in process_inputs
    self._validate_model_inputs(encoder_inputs, decoder_inputs)
  File "/home/himanshu/dev/code/.venv_phi4_req/lib64/python3.11/site-packages/vllm/v1/engine/processor.py", line 517, in _validate_model_inputs
    self._validate_model_input(decoder_inputs, prompt_type="decoder")
  File "/home/himanshu/dev/code/.venv_phi4_req/lib64/python3.11/site-packages/vllm/v1/engine/processor.py", line 591, in _validate_model_input
    raise ValueError(
ValueError: The decoder prompt (length 7041) is longer than the maximum model length of 4096. Make sure that `max_model_len` is no smaller than the number of text tokens plus multimodal tokens. For image inputs, the number of image tokens depends on the number of images, and possibly their aspect ratios as well.

2025-11-23 04:40:56,601 [ERROR]     Q1[0] individual error: The decoder prompt (length 7041) is longer than the maximum model length of 4096. Make sure that `max_model_len` is no smaller than the number of text tokens plus multimodal tokens. For image inputs, the number of image tokens depends on the number of images, and possibly their aspect ratios as well.
2025-11-23 04:40:57,300 [ERROR]     Q1[1] individual error: The decoder prompt (length 7780) is longer than the maximum model length of 4096. Make sure that `max_model_len` is no smaller than the number of text tokens plus multimodal tokens. For image inputs, the number of image tokens depends on the number of images, and possibly their aspect ratios as well.
2025-11-23 04:40:57,888 [ERROR]     Q1[2] individual error: The decoder prompt (length 7982) is longer than the maximum model length of 4096. Make sure that `max_model_len` is no smaller than the number of text tokens plus multimodal tokens. For image inputs, the number of image tokens depends on the number of images, and possibly their aspect ratios as well.
2025-11-23 04:40:57,889 [INFO]   Batch Q2-Q4: Generating 9 text questions with RAG...
2025-11-23 04:41:18,853 [INFO]     Q2[0] done: 20.53s (RAG: True, chunks: 5)
2025-11-23 04:41:18,853 [INFO]     Q3[0] done: 20.53s (RAG: True, chunks: 5)
2025-11-23 04:41:18,853 [INFO]     Q4[0] done: 20.53s (RAG: True, chunks: 5)
2025-11-23 04:41:18,853 [INFO]     Q2[1] done: 20.53s (RAG: True, chunks: 5)
2025-11-23 04:41:18,854 [INFO]     Q3[1] done: 20.53s (RAG: True, chunks: 5)
2025-11-23 04:41:18,854 [INFO]     Q4[1] done: 20.53s (RAG: True, chunks: 5)
2025-11-23 04:41:18,854 [INFO]     Q2[2] done: 20.53s (RAG: True, chunks: 5)
2025-11-23 04:41:18,854 [INFO]     Q3[2] done: 20.53s (RAG: True, chunks: 5)
2025-11-23 04:41:18,854 [INFO]     Q4[2] done: 20.53s (RAG: True, chunks: 5)
2025-11-23 04:41:18,854 [INFO]   ✓ Saved 100_Nicotine__answers.json
2025-11-23 04:41:18,855 [INFO]   ✓ Saved 101_Nitric_Acid__answers.json
2025-11-23 04:41:18,855 [INFO]   ✓ Saved 102_Nitric_Oxide__answers.json
2025-11-23 04:41:18,855 [INFO] 
======================================================================
2025-11-23 04:41:18,856 [INFO] REGENERATION SUMMARY
2025-11-23 04:41:18,856 [INFO] ======================================================================
2025-11-23 04:41:18,856 [INFO] Total files: 3
2025-11-23 04:41:18,856 [INFO] Successful: 3
2025-11-23 04:41:18,856 [INFO] Failed: 0
2025-11-23 04:41:18,856 [INFO] Total time: 30.12s (0.50 min)
2025-11-23 04:41:18,856 [INFO] Average per file: 10.04s
2025-11-23 04:41:18,856 [INFO] vLLM used: True
2025-11-23 04:41:18,856 [INFO] Summary saved to: /home/himanshu/dev/output/qwen_rag/rag_regeneration_summary.json
2025-11-23 04:41:18,856 [INFO] ======================================================================
