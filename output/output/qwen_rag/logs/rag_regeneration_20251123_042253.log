2025-11-23 04:22:53,231 [INFO] ======================================================================
2025-11-23 04:22:53,231 [INFO] QWEN Answer Regeneration with vLLM + RAG
2025-11-23 04:22:53,231 [INFO] ======================================================================
2025-11-23 04:22:53,231 [INFO] Input directory: /home/himanshu/dev/output/qwen_regenerated
2025-11-23 04:22:53,231 [INFO] Output directory: /home/himanshu/dev/output/qwen_rag
2025-11-23 04:22:53,231 [INFO] QA directory: /home/himanshu/dev/test/data/processed/qa_pairs_individual_components
2025-11-23 04:22:53,231 [INFO] Max tokens: 500
2025-11-23 04:22:53,231 [INFO] vLLM available: True
2025-11-23 04:22:53,231 [INFO] ChromaDB available: True
2025-11-23 04:22:53,231 [INFO] ChromaDB path: /home/himanshu/dev/data/chromadb
2025-11-23 04:22:53,231 [INFO] Test limit: 3
2025-11-23 04:22:53,231 [INFO] ======================================================================
2025-11-23 04:22:53,231 [INFO] Initializing model...
2025-11-23 04:22:54,212 [INFO] Initializing ChromaDB search system from /home/himanshu/dev/data/chromadb...
2025-11-23 04:22:57,701 [INFO] ChromaDB search system initialized successfully
2025-11-23 04:22:57,701 [INFO] Initializing vLLM for ALL questions (text and vision)...
2025-11-23 04:22:57,701 [INFO] vLLM is REQUIRED - no Transformers fallback available
2025-11-23 04:22:57,701 [INFO] Environment variables set: VLLM_SKIP_MM_PROFILE=1, SKIP_MM_PROFILE=1
2025-11-23 04:22:57,716 [ERROR] CRITICAL: Failed to initialize vLLM: 1 validation error for ModelConfig
limit_mm_per_prompt
  Input should be a valid dictionary [type=dict_type, input_value=1, input_type=int]
    For further information visit https://errors.pydantic.dev/2.12/v/dict_type
2025-11-23 04:22:57,717 [ERROR] vLLM initialization error details: Traceback (most recent call last):
  File "/home/himanshu/dev/code/multimodal_qa_runner_vllm.py", line 124, in __init__
    self.vllm_llm = LLM(
                    ^^^^
  File "/home/himanshu/dev/code/.venv_phi4_req/lib64/python3.11/site-packages/vllm/entrypoints/llm.py", line 343, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/himanshu/dev/code/.venv_phi4_req/lib64/python3.11/site-packages/vllm/v1/engine/llm_engine.py", line 166, in from_engine_args
    vllm_config = engine_args.create_engine_config(usage_context)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/himanshu/dev/code/.venv_phi4_req/lib64/python3.11/site-packages/vllm/engine/arg_utils.py", line 1360, in create_engine_config
    model_config = self.create_model_config()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/himanshu/dev/code/.venv_phi4_req/lib64/python3.11/site-packages/vllm/engine/arg_utils.py", line 1215, in create_model_config
    return ModelConfig(
           ^^^^^^^^^^^^
  File "/home/himanshu/dev/code/.venv_phi4_req/lib64/python3.11/site-packages/pydantic/_internal/_dataclasses.py", line 121, in __init__
    s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)
pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig
limit_mm_per_prompt
  Input should be a valid dictionary [type=dict_type, input_value=1, input_type=int]
    For further information visit https://errors.pydantic.dev/2.12/v/dict_type

2025-11-23 04:22:57,718 [ERROR] vLLM is REQUIRED - cannot proceed without it. Exiting.
