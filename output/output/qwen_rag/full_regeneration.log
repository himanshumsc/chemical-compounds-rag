/home/himanshu/dev/code/.venv_phi4_req/lib64/python3.11/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2025-11-23 04:15:02,610 [INFO] ======================================================================
[2025-11-23 04:15:02] INFO multimodal_qa_runner_vllm.py:502: ======================================================================
2025-11-23 04:15:02,610 [INFO] QWEN Answer Regeneration with vLLM + RAG
[2025-11-23 04:15:02] INFO multimodal_qa_runner_vllm.py:503: QWEN Answer Regeneration with vLLM + RAG
2025-11-23 04:15:02,610 [INFO] ======================================================================
[2025-11-23 04:15:02] INFO multimodal_qa_runner_vllm.py:504: ======================================================================
2025-11-23 04:15:02,610 [INFO] Input directory: /home/himanshu/dev/output/qwen_regenerated
[2025-11-23 04:15:02] INFO multimodal_qa_runner_vllm.py:505: Input directory: /home/himanshu/dev/output/qwen_regenerated
2025-11-23 04:15:02,610 [INFO] Output directory: /home/himanshu/dev/output/qwen_rag
[2025-11-23 04:15:02] INFO multimodal_qa_runner_vllm.py:506: Output directory: /home/himanshu/dev/output/qwen_rag
2025-11-23 04:15:02,611 [INFO] QA directory: /home/himanshu/dev/test/data/processed/qa_pairs_individual_components
[2025-11-23 04:15:02] INFO multimodal_qa_runner_vllm.py:507: QA directory: /home/himanshu/dev/test/data/processed/qa_pairs_individual_components
2025-11-23 04:15:02,611 [INFO] Max tokens: 500
[2025-11-23 04:15:02] INFO multimodal_qa_runner_vllm.py:508: Max tokens: 500
2025-11-23 04:15:02,611 [INFO] vLLM available: True
[2025-11-23 04:15:02] INFO multimodal_qa_runner_vllm.py:509: vLLM available: True
2025-11-23 04:15:02,611 [INFO] ChromaDB available: True
[2025-11-23 04:15:02] INFO multimodal_qa_runner_vllm.py:510: ChromaDB available: True
2025-11-23 04:15:02,611 [INFO] ChromaDB path: /home/himanshu/dev/data/chromadb
[2025-11-23 04:15:02] INFO multimodal_qa_runner_vllm.py:511: ChromaDB path: /home/himanshu/dev/data/chromadb
2025-11-23 04:15:02,611 [INFO] Test limit: All files
[2025-11-23 04:15:02] INFO multimodal_qa_runner_vllm.py:512: Test limit: All files
2025-11-23 04:15:02,611 [INFO] ======================================================================
[2025-11-23 04:15:02] INFO multimodal_qa_runner_vllm.py:513: ======================================================================
2025-11-23 04:15:02,611 [INFO] Initializing model...
[2025-11-23 04:15:02] INFO multimodal_qa_runner_vllm.py:516: Initializing model...
The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.
2025-11-23 04:15:03,606 [INFO] Initializing ChromaDB search system from /home/himanshu/dev/data/chromadb...
[2025-11-23 04:15:03] INFO multimodal_qa_runner_vllm.py:95: Initializing ChromaDB search system from /home/himanshu/dev/data/chromadb...
[2025-11-23 04:15:03] INFO chromadb_search.py:46: Loading CLIP model...
[2025-11-23 04:15:06] INFO chromadb_search.py:48: CLIP model loaded successfully
[2025-11-23 04:15:06] INFO chromadb_search.py:56: Loading ChromaDB from /home/himanshu/dev/data/chromadb
[2025-11-23 04:15:07] INFO chromadb_search.py:69: ChromaDB collection loaded successfully
[2025-11-23 04:15:07] INFO chromadb_search.py:73: Collection contains 100 documents
2025-11-23 04:15:07,104 [INFO] ChromaDB search system initialized successfully
[2025-11-23 04:15:07] INFO multimodal_qa_runner_vllm.py:97: ChromaDB search system initialized successfully
2025-11-23 04:15:07,104 [INFO] Initializing vLLM for ALL questions (text and vision)...
[2025-11-23 04:15:07] INFO multimodal_qa_runner_vllm.py:110: Initializing vLLM for ALL questions (text and vision)...
2025-11-23 04:15:07,104 [INFO] vLLM is REQUIRED - no Transformers fallback available
[2025-11-23 04:15:07] INFO multimodal_qa_runner_vllm.py:111: vLLM is REQUIRED - no Transformers fallback available
INFO 11-23 04:15:07 [utils.py:253] non-default args: {'trust_remote_code': True, 'dtype': 'float16', 'max_model_len': 4096, 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'model': '/home/himanshu/dev/models/QWEN_AWQ'}
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
INFO 11-23 04:15:07 [model.py:631] Resolved architecture: Qwen2_5_VLForConditionalGeneration
WARNING 11-23 04:15:07 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 11-23 04:15:07 [model.py:1745] Using max model len 4096
INFO 11-23 04:15:07 [awq_marlin.py:162] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
INFO 11-23 04:15:07 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 11-23 04:15:07 [vllm.py:500] Cudagraph is disabled under eager mode
[1;36m(EngineCore_DP0 pid=154222)[0;0m INFO 11-23 04:15:09 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='/home/himanshu/dev/models/QWEN_AWQ', speculative_config=None, tokenizer='/home/himanshu/dev/models/QWEN_AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=awq_marlin, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/himanshu/dev/models/QWEN_AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': None, 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 0, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=154222)[0;0m INFO 11-23 04:15:09 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.148.0.2:47735 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=154222)[0;0m INFO 11-23 04:15:09 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
