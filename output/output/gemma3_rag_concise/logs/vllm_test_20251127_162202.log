/home/himanshu/dev/code/.venv_phi4_req/lib64/python3.11/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
======================================================================
Gemma-3 GGUF vLLM smoke test (dry run until executed manually)
======================================================================
Model path : /home/himanshu/dev/models/GEMMA3_QAT_Q4_0_UNQUANTIZED
Tokenizer  : google/gemma-3-12b-it
Max len    : 8192
GPU util   : 0.8
INFO 11-27 16:22:15 [utils.py:253] non-default args: {'tokenizer': 'google/gemma-3-12b-it', 'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': 8192, 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'quantization': 'bitsandbytes', 'enforce_eager': True, 'model': '/home/himanshu/dev/models/GEMMA3_QAT_Q4_0_UNQUANTIZED'}
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
INFO 11-27 16:22:15 [model.py:631] Resolved architecture: Gemma3ForConditionalGeneration
INFO 11-27 16:22:15 [model.py:1745] Using max model len 8192
INFO 11-27 16:22:15 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 11-27 16:22:16 [vllm.py:500] Cudagraph is disabled under eager mode
[1;36m(EngineCore_DP0 pid=74877)[0;0m INFO 11-27 16:22:22 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='/home/himanshu/dev/models/GEMMA3_QAT_Q4_0_UNQUANTIZED', speculative_config=None, tokenizer='google/gemma-3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/himanshu/dev/models/GEMMA3_QAT_Q4_0_UNQUANTIZED, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': None, 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 0, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=74877)[0;0m INFO 11-27 16:22:24 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.148.0.2:49357 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=74877)[0;0m INFO 11-27 16:22:24 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=74877)[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[1;36m(EngineCore_DP0 pid=74877)[0;0m INFO 11-27 16:22:38 [gpu_model_runner.py:3259] Starting to load model /home/himanshu/dev/models/GEMMA3_QAT_Q4_0_UNQUANTIZED...
[1;36m(EngineCore_DP0 pid=74877)[0;0m INFO 11-27 16:22:39 [layer.py:570] MultiHeadAttention attn_backend: AttentionBackendEnum.XFORMERS, use_upstream_fa: False
[1;36m(EngineCore_DP0 pid=74877)[0;0m INFO 11-27 16:22:39 [vllm.py:500] Cudagraph is disabled under eager mode
[1;36m(EngineCore_DP0 pid=74877)[0;0m [2025-11-27 16:22:39] INFO _optional_torch_c_dlpack.py:88: JIT-compiling torch-c-dlpack-ext to cache...
[1;36m(EngineCore_DP0 pid=74877)[0;0m /home/himanshu/dev/code/.venv_phi4_req/lib64/python3.11/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:129: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
[1;36m(EngineCore_DP0 pid=74877)[0;0m We recommend installing via `pip install torch-c-dlpack-ext`
[1;36m(EngineCore_DP0 pid=74877)[0;0m   warnings.warn(
[1;36m(EngineCore_DP0 pid=74877)[0;0m INFO 11-27 16:22:41 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=74877)[0;0m INFO 11-27 16:22:41 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=74877)[0;0m INFO 11-27 16:22:41 [bitsandbytes_loader.py:791] Loading weights with BitsAndBytes quantization. May take a while ...
[1;36m(EngineCore_DP0 pid=74877)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=74877)[0;0m Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:28<01:53, 28.41s/it]
[1;36m(EngineCore_DP0 pid=74877)[0;0m Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:56<01:24, 28.17s/it]
[1;36m(EngineCore_DP0 pid=74877)[0;0m Loading safetensors checkpoint shards:  60% Completed | 3/5 [01:24<00:56, 28.09s/it]
[1;36m(EngineCore_DP0 pid=74877)[0;0m Loading safetensors checkpoint shards:  80% Completed | 4/5 [01:50<00:27, 27.32s/it]
[1;36m(EngineCore_DP0 pid=74877)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [02:18<00:00, 27.56s/it]
[1;36m(EngineCore_DP0 pid=74877)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [02:18<00:00, 27.71s/it]
[1;36m(EngineCore_DP0 pid=74877)[0;0m 
[1;36m(EngineCore_DP0 pid=74877)[0;0m INFO 11-27 16:25:01 [gpu_model_runner.py:3338] Model loading took 8.3283 GiB memory and 141.888729 seconds
[1;36m(EngineCore_DP0 pid=74877)[0;0m INFO 11-27 16:25:01 [gpu_model_runner.py:4088] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 31 image items of the maximum feature size.
[1;36m(EngineCore_DP0 pid=74877)[0;0m INFO 11-27 16:25:17 [gpu_worker.py:359] Available KV cache memory: 4.80 GiB
[1;36m(EngineCore_DP0 pid=74877)[0;0m INFO 11-27 16:25:18 [kv_cache_utils.py:1229] GPU KV cache size: 13,104 tokens
[1;36m(EngineCore_DP0 pid=74877)[0;0m INFO 11-27 16:25:18 [kv_cache_utils.py:1234] Maximum concurrency for 8,192 tokens per request: 1.60x
[1;36m(EngineCore_DP0 pid=74877)[0;0m INFO 11-27 16:25:18 [core.py:250] init engine (profile, create kv cache, warmup model) took 17.38 seconds
[1;36m(EngineCore_DP0 pid=74877)[0;0m INFO 11-27 16:25:21 [vllm.py:500] Cudagraph is disabled under eager mode
INFO 11-27 16:25:22 [llm.py:352] Supported tasks: ['generate']
âœ“ vLLM initialized in 186.74s
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 444.59it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][1;36m(EngineCore_DP0 pid=74877)[0;0m [rank0]:W1127 16:25:22.310000 74877 .venv_phi4_req/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:1358] [0/8] torch._dynamo hit config.recompile_limit (8)
[1;36m(EngineCore_DP0 pid=74877)[0;0m [rank0]:W1127 16:25:22.310000 74877 .venv_phi4_req/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:1358] [0/8]    function: 'forward_static' (/home/himanshu/dev/code/.venv_phi4_req/lib64/python3.11/site-packages/vllm/model_executor/layers/layernorm.py:274)
[1;36m(EngineCore_DP0 pid=74877)[0;0m [rank0]:W1127 16:25:22.310000 74877 .venv_phi4_req/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:1358] [0/8]    last reason: 0/7: expected type of 'residual' to be a tensor type, ' but found <class 'NoneType'>
[1;36m(EngineCore_DP0 pid=74877)[0;0m [rank0]:W1127 16:25:22.310000 74877 .venv_phi4_req/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:1358] [0/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[1;36m(EngineCore_DP0 pid=74877)[0;0m [rank0]:W1127 16:25:22.310000 74877 .venv_phi4_req/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:1358] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.15s/it, est. speed input: 0.65 toks/s, output: 6.35 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.15s/it, est. speed input: 0.65 toks/s, output: 6.35 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.15s/it, est. speed input: 0.65 toks/s, output: 6.35 toks/s]

=== Sample Output ===
Here are three safety precautions when handling acetic acid in a lab:

1.  **Wear appropriate Personal Protective Equipment (PPE):** This includes safety goggles (to protect eyes from splashes), gloves (nitrile or neoprene are recommended as acetic acid can dissolve some latex gloves), and a lab coat (to protect clothing and skin).
2.  **Work in a well-ventilated area or fume hood:** Acetic acid has a strong, irritating odor.  Working in a fume hood minimizes exposure to vapors, reducing respiratory irritation.
3.  **Handle with care and avoid spills:**  Acetic acid is corrosive.
=====================
