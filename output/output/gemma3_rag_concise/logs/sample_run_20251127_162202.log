/home/himanshu/dev/code/.venv_phi4_req/lib64/python3.11/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2025-11-27 16:25:59,912 [INFO] ======================================================================
[2025-11-27 16:25:59] INFO multimodal_qa_runner_vllm.py:714: ======================================================================
2025-11-27 16:25:59,912 [INFO] QWEN Answer Regeneration with vLLM + RAG
[2025-11-27 16:25:59] INFO multimodal_qa_runner_vllm.py:715: QWEN Answer Regeneration with vLLM + RAG
2025-11-27 16:25:59,912 [INFO] ======================================================================
[2025-11-27 16:25:59] INFO multimodal_qa_runner_vllm.py:716: ======================================================================
2025-11-27 16:25:59,912 [INFO] Input directory: /home/himanshu/dev/output/qwen_regenerated
[2025-11-27 16:25:59] INFO multimodal_qa_runner_vllm.py:717: Input directory: /home/himanshu/dev/output/qwen_regenerated
2025-11-27 16:25:59,912 [INFO] Output directory: /home/himanshu/dev/output/gemma3_rag_concise
[2025-11-27 16:25:59] INFO multimodal_qa_runner_vllm.py:718: Output directory: /home/himanshu/dev/output/gemma3_rag_concise
2025-11-27 16:25:59,912 [INFO] QA directory: /home/himanshu/dev/test/data/processed/qa_pairs_individual_components
[2025-11-27 16:25:59] INFO multimodal_qa_runner_vllm.py:719: QA directory: /home/himanshu/dev/test/data/processed/qa_pairs_individual_components
2025-11-27 16:25:59,912 [INFO] Model path: /home/himanshu/dev/models/GEMMA3_QAT_Q4_0_UNQUANTIZED
[2025-11-27 16:25:59] INFO multimodal_qa_runner_vllm.py:720: Model path: /home/himanshu/dev/models/GEMMA3_QAT_Q4_0_UNQUANTIZED
2025-11-27 16:25:59,912 [INFO] Tokenizer id: google/gemma-3-12b-it
[2025-11-27 16:25:59] INFO multimodal_qa_runner_vllm.py:721: Tokenizer id: google/gemma-3-12b-it
2025-11-27 16:25:59,912 [INFO] Quantization: bitsandbytes | dtype: bfloat16 | max model len: 8192 | gpu util: 0.8 | enforce eager: True
[2025-11-27 16:25:59] INFO multimodal_qa_runner_vllm.py:722: Quantization: bitsandbytes | dtype: bfloat16 | max model len: 8192 | gpu util: 0.8 | enforce eager: True
2025-11-27 16:25:59,912 [INFO] Character limits: Q1=600, Q2=1000, Q3=1800, Q4=2000
[2025-11-27 16:25:59] INFO multimodal_qa_runner_vllm.py:723: Character limits: Q1=600, Q2=1000, Q3=1800, Q4=2000
2025-11-27 16:25:59,912 [INFO] Max tokens: Q1=200, Q2=333, Q3=600, Q4=666
[2025-11-27 16:25:59] INFO multimodal_qa_runner_vllm.py:724: Max tokens: Q1=200, Q2=333, Q3=600, Q4=666
2025-11-27 16:25:59,912 [INFO] Prompt instruction: Answers should be brief, concise, and to the point
[2025-11-27 16:25:59] INFO multimodal_qa_runner_vllm.py:725: Prompt instruction: Answers should be brief, concise, and to the point
2025-11-27 16:25:59,912 [INFO] vLLM available: True
[2025-11-27 16:25:59] INFO multimodal_qa_runner_vllm.py:726: vLLM available: True
2025-11-27 16:25:59,912 [INFO] ChromaDB available: True
[2025-11-27 16:25:59] INFO multimodal_qa_runner_vllm.py:727: ChromaDB available: True
2025-11-27 16:25:59,912 [INFO] ChromaDB path: /home/himanshu/dev/data/chromadb
[2025-11-27 16:25:59] INFO multimodal_qa_runner_vllm.py:728: ChromaDB path: /home/himanshu/dev/data/chromadb
2025-11-27 16:25:59,913 [INFO] Test limit: 2
[2025-11-27 16:25:59] INFO multimodal_qa_runner_vllm.py:729: Test limit: 2
2025-11-27 16:25:59,913 [INFO] ======================================================================
[2025-11-27 16:25:59] INFO multimodal_qa_runner_vllm.py:730: ======================================================================
2025-11-27 16:25:59,913 [INFO] vLLM is available - proceeding with initialization...
[2025-11-27 16:25:59] INFO multimodal_qa_runner_vllm.py:738: vLLM is available - proceeding with initialization...
2025-11-27 16:25:59,913 [INFO] Initializing model...
[2025-11-27 16:25:59] INFO multimodal_qa_runner_vllm.py:742: Initializing model...
2025-11-27 16:25:59,913 [INFO] Initializing vLLM for ALL questions (text and vision)...
[2025-11-27 16:25:59] INFO multimodal_qa_runner_vllm.py:135: Initializing vLLM for ALL questions (text and vision)...
2025-11-27 16:25:59,913 [INFO] vLLM is REQUIRED - no Transformers fallback available
[2025-11-27 16:25:59] INFO multimodal_qa_runner_vllm.py:136: vLLM is REQUIRED - no Transformers fallback available
2025-11-27 16:25:59,913 [INFO] Multimodal profiling ENABLED (required for Q1 image questions)
[2025-11-27 16:25:59] INFO multimodal_qa_runner_vllm.py:137: Multimodal profiling ENABLED (required for Q1 image questions)
2025-11-27 16:25:59,913 [INFO] Environment variables: VLLM_SKIP_MM_PROFILE=not set, SKIP_MM_PROFILE=not set
[2025-11-27 16:25:59] INFO multimodal_qa_runner_vllm.py:138: Environment variables: VLLM_SKIP_MM_PROFILE=not set, SKIP_MM_PROFILE=not set
2025-11-27 16:25:59,913 [INFO] Starting vLLM initialization (this may take 20-30 seconds)...
[2025-11-27 16:25:59] INFO multimodal_qa_runner_vllm.py:142: Starting vLLM initialization (this may take 20-30 seconds)...
INFO 11-27 16:25:59 [utils.py:253] non-default args: {'tokenizer': 'google/gemma-3-12b-it', 'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': 8192, 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'quantization': 'bitsandbytes', 'enforce_eager': True, 'model': '/home/himanshu/dev/models/GEMMA3_QAT_Q4_0_UNQUANTIZED'}
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
INFO 11-27 16:25:59 [model.py:631] Resolved architecture: Gemma3ForConditionalGeneration
INFO 11-27 16:25:59 [model.py:1745] Using max model len 8192
INFO 11-27 16:26:00 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 11-27 16:26:00 [vllm.py:500] Cudagraph is disabled under eager mode
[1;36m(EngineCore_DP0 pid=76154)[0;0m INFO 11-27 16:26:07 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='/home/himanshu/dev/models/GEMMA3_QAT_Q4_0_UNQUANTIZED', speculative_config=None, tokenizer='google/gemma-3-12b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/himanshu/dev/models/GEMMA3_QAT_Q4_0_UNQUANTIZED, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': None, 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 0, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=76154)[0;0m INFO 11-27 16:26:09 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.148.0.2:46837 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=76154)[0;0m INFO 11-27 16:26:09 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=76154)[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[1;36m(EngineCore_DP0 pid=76154)[0;0m INFO 11-27 16:26:23 [gpu_model_runner.py:3259] Starting to load model /home/himanshu/dev/models/GEMMA3_QAT_Q4_0_UNQUANTIZED...
[1;36m(EngineCore_DP0 pid=76154)[0;0m INFO 11-27 16:26:24 [layer.py:570] MultiHeadAttention attn_backend: AttentionBackendEnum.XFORMERS, use_upstream_fa: False
[1;36m(EngineCore_DP0 pid=76154)[0;0m INFO 11-27 16:26:24 [vllm.py:500] Cudagraph is disabled under eager mode
[1;36m(EngineCore_DP0 pid=76154)[0;0m [2025-11-27 16:26:24] INFO _optional_torch_c_dlpack.py:88: JIT-compiling torch-c-dlpack-ext to cache...
[1;36m(EngineCore_DP0 pid=76154)[0;0m /home/himanshu/dev/code/.venv_phi4_req/lib64/python3.11/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:129: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
[1;36m(EngineCore_DP0 pid=76154)[0;0m We recommend installing via `pip install torch-c-dlpack-ext`
[1;36m(EngineCore_DP0 pid=76154)[0;0m   warnings.warn(
[1;36m(EngineCore_DP0 pid=76154)[0;0m INFO 11-27 16:26:26 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=76154)[0;0m INFO 11-27 16:26:26 [cuda.py:427] Using FLASH_ATTN backend.
[1;36m(EngineCore_DP0 pid=76154)[0;0m INFO 11-27 16:26:26 [bitsandbytes_loader.py:791] Loading weights with BitsAndBytes quantization. May take a while ...
[1;36m(EngineCore_DP0 pid=76154)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=76154)[0;0m Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:28<01:53, 28.42s/it]
[1;36m(EngineCore_DP0 pid=76154)[0;0m Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:56<01:24, 28.18s/it]
[1;36m(EngineCore_DP0 pid=76154)[0;0m Loading safetensors checkpoint shards:  60% Completed | 3/5 [01:24<00:56, 28.08s/it]
[1;36m(EngineCore_DP0 pid=76154)[0;0m Loading safetensors checkpoint shards:  80% Completed | 4/5 [01:50<00:27, 27.30s/it]
[1;36m(EngineCore_DP0 pid=76154)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [02:18<00:00, 27.52s/it]
[1;36m(EngineCore_DP0 pid=76154)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [02:18<00:00, 27.68s/it]
[1;36m(EngineCore_DP0 pid=76154)[0;0m 
[1;36m(EngineCore_DP0 pid=76154)[0;0m INFO 11-27 16:28:46 [gpu_model_runner.py:3338] Model loading took 8.3283 GiB memory and 141.697732 seconds
[1;36m(EngineCore_DP0 pid=76154)[0;0m INFO 11-27 16:28:46 [gpu_model_runner.py:4088] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 31 image items of the maximum feature size.
[1;36m(EngineCore_DP0 pid=76154)[0;0m INFO 11-27 16:29:02 [gpu_worker.py:359] Available KV cache memory: 4.80 GiB
[1;36m(EngineCore_DP0 pid=76154)[0;0m INFO 11-27 16:29:03 [kv_cache_utils.py:1229] GPU KV cache size: 13,104 tokens
[1;36m(EngineCore_DP0 pid=76154)[0;0m INFO 11-27 16:29:03 [kv_cache_utils.py:1234] Maximum concurrency for 8,192 tokens per request: 1.60x
[1;36m(EngineCore_DP0 pid=76154)[0;0m INFO 11-27 16:29:03 [core.py:250] init engine (profile, create kv cache, warmup model) took 17.56 seconds
[1;36m(EngineCore_DP0 pid=76154)[0;0m INFO 11-27 16:29:06 [vllm.py:500] Cudagraph is disabled under eager mode
INFO 11-27 16:29:07 [llm.py:352] Supported tasks: ['generate']
2025-11-27 16:29:07,140 [INFO] vLLM config -> tokenizer:google/gemma-3-12b-it quantization:bitsandbytes dtype:bfloat16 max_len:8192 gpu_util:0.80 enforce_eager:True
[2025-11-27 16:29:07] INFO multimodal_qa_runner_vllm.py:159: vLLM config -> tokenizer:google/gemma-3-12b-it quantization:bitsandbytes dtype:bfloat16 max_len:8192 gpu_util:0.80 enforce_eager:True
2025-11-27 16:29:07,141 [INFO] vLLM initialized successfully for all question types
[2025-11-27 16:29:07] INFO multimodal_qa_runner_vllm.py:190: vLLM initialized successfully for all question types
2025-11-27 16:29:07,141 [INFO] Character limits: Q1=600, Q2=1000, Q3=1800, Q4=2000
[2025-11-27 16:29:07] INFO multimodal_qa_runner_vllm.py:191: Character limits: Q1=600, Q2=1000, Q3=1800, Q4=2000
2025-11-27 16:29:07,142 [INFO] Max tokens: Q1=200, Q2=333, Q3=600, Q4=666
[2025-11-27 16:29:07] INFO multimodal_qa_runner_vllm.py:192: Max tokens: Q1=200, Q2=333, Q3=600, Q4=666
2025-11-27 16:29:07,142 [INFO] Initializing processor for chat template formatting...
[2025-11-27 16:29:07] INFO multimodal_qa_runner_vllm.py:203: Initializing processor for chat template formatting...
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
2025-11-27 16:29:11,282 [INFO] Processor initialized successfully
[2025-11-27 16:29:11] INFO multimodal_qa_runner_vllm.py:205: Processor initialized successfully
2025-11-27 16:29:11,283 [INFO] Initializing ChromaDB search system from /home/himanshu/dev/data/chromadb...
[2025-11-27 16:29:11] INFO multimodal_qa_runner_vllm.py:217: Initializing ChromaDB search system from /home/himanshu/dev/data/chromadb...
[2025-11-27 16:29:11] INFO chromadb_search.py:46: Loading CLIP model...
[2025-11-27 16:29:17] INFO chromadb_search.py:48: CLIP model loaded successfully
[2025-11-27 16:29:17] INFO chromadb_search.py:56: Loading ChromaDB from /home/himanshu/dev/data/chromadb
[2025-11-27 16:29:18] INFO chromadb_search.py:69: ChromaDB collection loaded successfully
[2025-11-27 16:29:18] INFO chromadb_search.py:73: Collection contains 100 documents
2025-11-27 16:29:18,343 [INFO] ChromaDB search system initialized successfully
[2025-11-27 16:29:18] INFO multimodal_qa_runner_vllm.py:219: ChromaDB search system initialized successfully
2025-11-27 16:29:18,343 [INFO] Model initialized. Using vLLM: True, RAG: True
[2025-11-27 16:29:18] INFO multimodal_qa_runner_vllm.py:759: Model initialized. Using vLLM: True, RAG: True
2025-11-27 16:29:18,344 [INFO] TEST MODE: Processing 2 files
[2025-11-27 16:29:18] INFO multimodal_qa_runner_vllm.py:775: TEST MODE: Processing 2 files
2025-11-27 16:29:18,344 [INFO] Total files to process: 2
[2025-11-27 16:29:18] INFO multimodal_qa_runner_vllm.py:777: Total files to process: 2
2025-11-27 16:29:18,344 [INFO] 
Processing batch 1 (1 files)
[2025-11-27 16:29:18] INFO multimodal_qa_runner_vllm.py:787: 
Processing batch 1 (1 files)
2025-11-27 16:29:18,345 [INFO] Loading: 100_Nicotine__answers.json (source: 100_Nicotine.json)
[2025-11-27 16:29:18] INFO multimodal_qa_runner_vllm.py:798: Loading: 100_Nicotine__answers.json (source: 100_Nicotine.json)
2025-11-27 16:29:18,940 [INFO]   Batch Q1: Generating 1 vision questions with RAG (char limit: 600)...
[2025-11-27 16:29:18] INFO multimodal_qa_runner_vllm.py:841:   Batch Q1: Generating 1 vision questions with RAG (char limit: 600)...
[2025-11-27 16:29:19] INFO chromadb_search.py:148: Image search: '/home/himanshu/dev/tmp/tmpw0k4epgq.png' (n_results=5)
[2025-11-27 16:29:19] INFO chromadb_search.py:175: Found 5 results
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.67s/it]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.67s/it]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][1;36m(EngineCore_DP0 pid=76154)[0;0m [rank0]:W1127 16:29:33.578000 76154 .venv_phi4_req/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:1358] [0/8] torch._dynamo hit config.recompile_limit (8)
[1;36m(EngineCore_DP0 pid=76154)[0;0m [rank0]:W1127 16:29:33.578000 76154 .venv_phi4_req/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:1358] [0/8]    function: 'forward_static' (/home/himanshu/dev/code/.venv_phi4_req/lib64/python3.11/site-packages/vllm/model_executor/layers/layernorm.py:274)
[1;36m(EngineCore_DP0 pid=76154)[0;0m [rank0]:W1127 16:29:33.578000 76154 .venv_phi4_req/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:1358] [0/8]    last reason: 0/7: expected type of 'residual' to be a tensor type, ' but found <class 'NoneType'>
[1;36m(EngineCore_DP0 pid=76154)[0;0m [rank0]:W1127 16:29:33.578000 76154 .venv_phi4_req/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:1358] [0/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
[1;36m(EngineCore_DP0 pid=76154)[0;0m [rank0]:W1127 16:29:33.578000 76154 .venv_phi4_req/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:1358] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html
Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:08<00:00,  8.93s/it, est. speed input: 231.97 toks/s, output: 5.38 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:08<00:00,  8.93s/it, est. speed input: 231.97 toks/s, output: 5.38 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:08<00:00,  8.93s/it, est. speed input: 231.97 toks/s, output: 5.38 toks/s]
2025-11-27 16:29:42,433 [INFO]     Q1[0] done: 22.60s (RAG: True, chunks: 5, len: 196 chars)
[2025-11-27 16:29:42] INFO multimodal_qa_runner_vllm.py:847:     Q1[0] done: 22.60s (RAG: True, chunks: 5, len: 196 chars)
2025-11-27 16:29:42,433 [INFO]   Batch Q2-Q4: Generating 3 text questions with RAG (char limits: Q2=1000, Q3=1800, Q4=2000)...
[2025-11-27 16:29:42] INFO multimodal_qa_runner_vllm.py:880:   Batch Q2-Q4: Generating 3 text questions with RAG (char limits: Q2=1000, Q3=1800, Q4=2000)...
[2025-11-27 16:29:42] INFO chromadb_search.py:113: Text search: 'What is the molecular formula of nicotine and what elements does it consist of?' (n_results=5)
[2025-11-27 16:29:42] INFO chromadb_search.py:137: Found 5 results
[2025-11-27 16:29:42] INFO chromadb_search.py:113: Text search: 'Describe the solubility properties of nicotine and the implications for its extraction process.' (n_results=5)
[2025-11-27 16:29:42] INFO chromadb_search.py:137: Found 5 results
[2025-11-27 16:29:42] INFO chromadb_search.py:113: Text search: 'Explain how the historical development and synthesis of nicotine have impacted its applications and uses.' (n_results=5)
[2025-11-27 16:29:42] INFO chromadb_search.py:137: Found 5 results
Adding requests:   0%|          | 0/3 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 179.99it/s]
Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:11<00:22, 11.41s/it, est. speed input: 173.83 toks/s, output: 3.33 toks/s]Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:11<00:04,  4.83s/it, est. speed input: 346.95 toks/s, output: 6.53 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:11<00:00,  4.83s/it, est. speed input: 532.88 toks/s, output: 9.80 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:11<00:00,  3.88s/it, est. speed input: 532.88 toks/s, output: 9.80 toks/s]
2025-11-27 16:29:54,253 [INFO]     Q2[0] done: 11.65s (RAG: True, chunks: 5, len: 217 chars)
[2025-11-27 16:29:54] INFO multimodal_qa_runner_vllm.py:893:     Q2[0] done: 11.65s (RAG: True, chunks: 5, len: 217 chars)
2025-11-27 16:29:54,253 [INFO]     Q3[0] done: 11.65s (RAG: True, chunks: 5, len: 213 chars)
[2025-11-27 16:29:54] INFO multimodal_qa_runner_vllm.py:893:     Q3[0] done: 11.65s (RAG: True, chunks: 5, len: 213 chars)
2025-11-27 16:29:54,254 [INFO]     Q4[0] done: 11.65s (RAG: True, chunks: 5, len: 209 chars)
[2025-11-27 16:29:54] INFO multimodal_qa_runner_vllm.py:893:     Q4[0] done: 11.65s (RAG: True, chunks: 5, len: 209 chars)
2025-11-27 16:29:54,254 [INFO]   âœ“ Saved 100_Nicotine__answers.json
[2025-11-27 16:29:54] INFO multimodal_qa_runner_vllm.py:1000:   âœ“ Saved 100_Nicotine__answers.json
2025-11-27 16:29:55,254 [INFO] 
Processing batch 2 (1 files)
[2025-11-27 16:29:55] INFO multimodal_qa_runner_vllm.py:787: 
Processing batch 2 (1 files)
2025-11-27 16:29:55,255 [INFO] Loading: 101_Nitric_Acid__answers.json (source: 101_Nitric_Acid.json)
[2025-11-27 16:29:55] INFO multimodal_qa_runner_vllm.py:798: Loading: 101_Nitric_Acid__answers.json (source: 101_Nitric_Acid.json)
2025-11-27 16:29:55,673 [INFO]   Batch Q1: Generating 1 vision questions with RAG (char limit: 600)...
[2025-11-27 16:29:55] INFO multimodal_qa_runner_vllm.py:841:   Batch Q1: Generating 1 vision questions with RAG (char limit: 600)...
[2025-11-27 16:29:55] INFO chromadb_search.py:148: Image search: '/home/himanshu/dev/tmp/tmpq5pr47n6.png' (n_results=5)
[2025-11-27 16:29:56] INFO chromadb_search.py:175: Found 5 results
Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.55it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.53it/s]
Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.34s/it, est. speed input: 296.65 toks/s, output: 5.25 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.34s/it, est. speed input: 296.65 toks/s, output: 5.25 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.34s/it, est. speed input: 296.65 toks/s, output: 5.25 toks/s]
2025-11-27 16:30:05,582 [INFO]     Q1[0] done: 9.44s (RAG: True, chunks: 5, len: 216 chars)
[2025-11-27 16:30:05] INFO multimodal_qa_runner_vllm.py:847:     Q1[0] done: 9.44s (RAG: True, chunks: 5, len: 216 chars)
2025-11-27 16:30:05,582 [INFO]   Batch Q2-Q4: Generating 3 text questions with RAG (char limits: Q2=1000, Q3=1800, Q4=2000)...
[2025-11-27 16:30:05] INFO multimodal_qa_runner_vllm.py:880:   Batch Q2-Q4: Generating 3 text questions with RAG (char limits: Q2=1000, Q3=1800, Q4=2000)...
[2025-11-27 16:30:05] INFO chromadb_search.py:113: Text search: 'What are the primary elements that make up nitric acid, and what is its chemical formula?' (n_results=5)
[2025-11-27 16:30:05] INFO chromadb_search.py:137: Found 5 results
[2025-11-27 16:30:05] INFO chromadb_search.py:113: Text search: 'Discuss the physical state and solubility characteristics of nitric acid.' (n_results=5)
[2025-11-27 16:30:05] INFO chromadb_search.py:137: Found 5 results
[2025-11-27 16:30:05] INFO chromadb_search.py:113: Text search: 'Explain why nitric acid is considered a strong oxidizing agent and describe its reaction with metals.' (n_results=5)
[2025-11-27 16:30:05] INFO chromadb_search.py:137: Found 5 results
Adding requests:   0%|          | 0/3 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 178.08it/s]
Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:08<00:17,  8.66s/it, est. speed input: 240.45 toks/s, output: 3.00 toks/s]Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:12<00:05,  5.62s/it, est. speed input: 325.49 toks/s, output: 5.52 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:25<00:00,  9.00s/it, est. speed input: 246.04 toks/s, output: 7.59 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:25<00:00,  9.00s/it, est. speed input: 246.04 toks/s, output: 7.59 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:25<00:00,  8.39s/it, est. speed input: 246.04 toks/s, output: 7.59 toks/s]
2025-11-27 16:30:30,895 [INFO]     Q2[0] done: 25.18s (RAG: True, chunks: 5, len: 126 chars)
[2025-11-27 16:30:30] INFO multimodal_qa_runner_vllm.py:893:     Q2[0] done: 25.18s (RAG: True, chunks: 5, len: 126 chars)
2025-11-27 16:30:30,895 [INFO]     Q3[0] done: 25.18s (RAG: True, chunks: 5, len: 223 chars)
[2025-11-27 16:30:30] INFO multimodal_qa_runner_vllm.py:893:     Q3[0] done: 25.18s (RAG: True, chunks: 5, len: 223 chars)
2025-11-27 16:30:30,895 [INFO]     Q4[0] done: 25.18s (RAG: True, chunks: 5, len: 549 chars)
[2025-11-27 16:30:30] INFO multimodal_qa_runner_vllm.py:893:     Q4[0] done: 25.18s (RAG: True, chunks: 5, len: 549 chars)
2025-11-27 16:30:30,895 [INFO]   âœ“ Saved 101_Nitric_Acid__answers.json
[2025-11-27 16:30:30] INFO multimodal_qa_runner_vllm.py:1000:   âœ“ Saved 101_Nitric_Acid__answers.json
2025-11-27 16:30:30,896 [INFO] 
======================================================================
[2025-11-27 16:30:30] INFO multimodal_qa_runner_vllm.py:1045: 
======================================================================
2025-11-27 16:30:30,896 [INFO] REGENERATION SUMMARY
[2025-11-27 16:30:30] INFO multimodal_qa_runner_vllm.py:1046: REGENERATION SUMMARY
2025-11-27 16:30:30,896 [INFO] ======================================================================
[2025-11-27 16:30:30] INFO multimodal_qa_runner_vllm.py:1047: ======================================================================
2025-11-27 16:30:30,896 [INFO] Total files: 2
[2025-11-27 16:30:30] INFO multimodal_qa_runner_vllm.py:1048: Total files: 2
2025-11-27 16:30:30,896 [INFO] Successful: 2
[2025-11-27 16:30:30] INFO multimodal_qa_runner_vllm.py:1049: Successful: 2
2025-11-27 16:30:30,896 [INFO] Failed: 0
[2025-11-27 16:30:30] INFO multimodal_qa_runner_vllm.py:1050: Failed: 0
2025-11-27 16:30:30,896 [INFO] Total time: 72.55s (1.21 min)
[2025-11-27 16:30:30] INFO multimodal_qa_runner_vllm.py:1051: Total time: 72.55s (1.21 min)
2025-11-27 16:30:30,896 [INFO] Average per file: 36.28s
[2025-11-27 16:30:30] INFO multimodal_qa_runner_vllm.py:1052: Average per file: 36.28s
2025-11-27 16:30:30,896 [INFO] vLLM used: True
[2025-11-27 16:30:30] INFO multimodal_qa_runner_vllm.py:1053: vLLM used: True
2025-11-27 16:30:30,896 [INFO] Summary saved to: /home/himanshu/dev/output/gemma3_rag_concise/rag_regeneration_summary.json
[2025-11-27 16:30:30] INFO multimodal_qa_runner_vllm.py:1054: Summary saved to: /home/himanshu/dev/output/gemma3_rag_concise/rag_regeneration_summary.json
2025-11-27 16:30:30,896 [INFO] ======================================================================
[2025-11-27 16:30:30] INFO multimodal_qa_runner_vllm.py:1055: ======================================================================
{
  "total_files": 2,
  "successful": 2,
  "failed": 0,
  "failed_files": [],
  "total_time_s": 72.55140209197998,
  "avg_per_file_s": 36.27570104598999,
  "vllm_used": true,
  "char_limit_q1": 600,
  "char_limit_q2": 1000,
  "char_limit_q3": 1800,
  "char_limit_q4": 2000,
  "max_tokens_q1": 200,
  "max_tokens_q2": 333,
  "max_tokens_q3": 600,
  "max_tokens_q4": 666,
  "concise_mode": true,
  "log_file": "/home/himanshu/dev/output/gemma3_rag_concise/logs/rag_regeneration_20251127_162559.log",
  "model_path": "/home/himanshu/dev/models/GEMMA3_QAT_Q4_0_UNQUANTIZED",
  "tokenizer_id": "google/gemma-3-12b-it",
  "quantization": "bitsandbytes",
  "dtype": "bfloat16",
  "max_model_len": 8192,
  "gpu_memory_utilization": 0.8,
  "enforce_eager": true
}
