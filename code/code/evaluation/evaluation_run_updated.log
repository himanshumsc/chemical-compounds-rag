2025-11-30 14:32:39 - INFO - Using virtual environment Python: /home/himanshu/dev/code/.venv_phi4_req/bin/python3
2025-11-30 14:32:39 - INFO - ======================================================================
2025-11-30 14:32:39 - INFO - Qwen vs Gemma RAG Concise - Complete Evaluation
2025-11-30 14:32:39 - INFO - ======================================================================
2025-11-30 14:32:39 - INFO - Start time: 2025-11-30 14:32:39
2025-11-30 14:32:39 - INFO - Results directory: /home/himanshu/dev/code/evaluation/results
2025-11-30 14:32:39 - INFO - Using Python: /home/himanshu/dev/code/.venv_phi4_req/bin/python3
2025-11-30 14:32:39 - INFO - ======================================================================
2025-11-30 14:32:39 - INFO - ======================================================================
2025-11-30 14:32:39 - INFO - Running BLEU Score Calculator...
2025-11-30 14:32:39 - INFO - ======================================================================
2025-11-30 14:32:41 - INFO - ======================================================================
2025-11-30 14:32:41 - INFO - BLEU Score Calculator: Qwen RAG Concise vs Gemma RAG Concise
2025-11-30 14:32:41 - INFO - ======================================================================
2025-11-30 14:32:41 - INFO - Qwen Directory: /home/himanshu/dev/output/qwen_rag_concise
2025-11-30 14:32:41 - INFO - Gemma Directory: /home/himanshu/dev/output/gemma3_rag_concise
2025-11-30 14:32:41 - INFO - OpenAI Directory: /home/himanshu/dev/test/data/processed/qa_pairs_individual_components_comprehensive
2025-11-30 14:32:41 - INFO - ======================================================================
2025-11-30 14:32:41 - INFO - Loading 178 Qwen answer files...
2025-11-30 14:32:41 - INFO - Loaded 178 Qwen files
2025-11-30 14:32:41 - INFO - Loading 178 Gemma answer files...
2025-11-30 14:32:41 - INFO - Loaded 178 Gemma files
2025-11-30 14:32:41 - INFO - Loading 178 OpenAI answer files...
2025-11-30 14:32:41 - INFO - Loaded 178 OpenAI files
2025-11-30 14:32:41 - INFO - Matching QA pairs...
2025-11-30 14:32:41 - INFO - Matched 712 question-answer triplets
2025-11-30 14:32:41 - INFO - Calculating BLEU scores...
2025-11-30 14:32:41 - INFO - Processed 100/712 pairs...
2025-11-30 14:32:42 - INFO - Processed 200/712 pairs...
2025-11-30 14:32:42 - INFO - Processed 300/712 pairs...
2025-11-30 14:32:42 - INFO - Processed 400/712 pairs...
2025-11-30 14:32:43 - INFO - Processed 500/712 pairs...
2025-11-30 14:32:43 - INFO - Processed 600/712 pairs...
2025-11-30 14:32:44 - INFO - Processed 700/712 pairs...
2025-11-30 14:32:44 - INFO - Completed BLEU calculation for 712 pairs
2025-11-30 14:32:44 - INFO - Calculating aggregated statistics...
2025-11-30 14:32:44 - INFO - Saving results...
2025-11-30 14:32:44 - INFO - Saved detailed results to: /home/himanshu/dev/code/evaluation/results/per_question_bleu_qwen_gemma.csv
2025-11-30 14:32:44 - INFO - Saved summary metrics to: /home/himanshu/dev/code/evaluation/results/summary_metrics_qwen_gemma.json
2025-11-30 14:32:44 - INFO - Saved comparison metrics to: /home/himanshu/dev/code/evaluation/results/comparison_bleu_qwen_gemma.json
2025-11-30 14:32:44 - INFO - BLEU score calculation complete!
2025-11-30 14:32:44 - INFO - Results saved to: /home/himanshu/dev/code/evaluation/results
2025-11-30 14:32:44 - INFO - Files: per_question_bleu_qwen_gemma.csv, summary_metrics_qwen_gemma.json, comparison_bleu_qwen_gemma.json

======================================================================
BLEU Score Summary - Qwen RAG Concise vs Gemma RAG Concise
======================================================================
Qwen: RAG Concise (character limits: Q1=600, Q2=1000, Q3=1800, Q4=2000)
Gemma: RAG Concise (character limits: Q1=600, Q2=1000, Q3=1800, Q4=2000)
OpenAI: Baseline (comprehensive)
======================================================================

Total Question-Answer Pairs: 712

----------------------------------------------------------------------
QWEN Overall Corpus BLEU Scores:
----------------------------------------------------------------------
  BLEU_1:
    Mean:   0.2425
    Median: 0.1722
    Std:    0.2200
    Range:  [0.0000, 1.0000]
  BLEU_2:
    Mean:   0.1709
    Median: 0.1066
    Std:    0.1879
    Range:  [0.0000, 1.0000]
  BLEU_3:
    Mean:   0.1213
    Median: 0.0628
    Std:    0.1668
    Range:  [0.0000, 1.0000]
  BLEU_4:
    Mean:   0.0906
    Median: 0.0363
    Std:    0.1517
    Range:  [0.0000, 1.0000]

----------------------------------------------------------------------
GEMMA Overall Corpus BLEU Scores:
----------------------------------------------------------------------
  BLEU_1:
    Mean:   0.1886
    Median: 0.1627
    Std:    0.1458
    Range:  [0.0002, 0.9500]
  BLEU_2:
    Mean:   0.1096
    Median: 0.0825
    Std:    0.1088
    Range:  [0.0001, 0.9220]
  BLEU_3:
    Mean:   0.0651
    Median: 0.0375
    Std:    0.0877
    Range:  [0.0000, 0.8924]
  BLEU_4:
    Mean:   0.0427
    Median: 0.0200
    Std:    0.0736
    Range:  [0.0000, 0.8579]

----------------------------------------------------------------------
COMPARISON: Qwen vs Gemma (BLEU-4)
----------------------------------------------------------------------
  Qwen Mean BLEU-4:  0.0906
  Gemma Mean BLEU-4: 0.0427
  Difference:        0.0479 (+4.79%)
  Winner:            Qwen

----------------------------------------------------------------------
Per-Question-Type Comparison (BLEU-4):
----------------------------------------------------------------------
  Q1:
    Qwen:  0.0317 (median: 0.0194)
    Gemma: 0.0300 (median: 0.0186)
    Diff:  0.0017 (+0.17%)
    Winner: Qwen
  Q2:
    Qwen:  0.2167 (median: 0.0969)
    Gemma: 0.0760 (median: 0.0291)
    Diff:  0.1407 (+14.07%)
    Winner: Qwen
  Q3:
    Qwen:  0.0732 (median: 0.0469)
    Gemma: 0.0468 (median: 0.0190)
    Diff:  0.0264 (+2.64%)
    Winner: Qwen
  Q4:
    Qwen:  0.0407 (median: 0.0213)
    Gemma: 0.0181 (median: 0.0076)
    Diff:  0.0227 (+2.27%)
    Winner: Qwen

----------------------------------------------------------------------
Answer Length Statistics:
----------------------------------------------------------------------
  QWEN:
    Mean:   218.7 chars
    Median: 200.0 chars
    Range:  [11, 697] chars
  GEMMA:
    Mean:   231.4 chars
    Median: 214.0 chars
    Range:  [47, 640] chars
  OPENAI:
    Mean:   532.5 chars
    Median: 516.0 chars
    Range:  [80, 1753] chars

======================================================================
2025-11-30 14:32:44 - INFO - ✅ BLEU Score Calculator completed successfully
2025-11-30 14:32:44 - INFO - ======================================================================
2025-11-30 14:32:44 - INFO - Running ROUGE Score Calculator...
2025-11-30 14:32:44 - INFO - ======================================================================
2025-11-30 14:32:45 - INFO - ======================================================================
2025-11-30 14:32:45 - INFO - ROUGE Score Calculator: Qwen RAG Concise vs Gemma RAG Concise
2025-11-30 14:32:45 - INFO - ======================================================================
2025-11-30 14:32:45 - INFO - Qwen Directory: /home/himanshu/dev/output/qwen_rag_concise
2025-11-30 14:32:45 - INFO - Gemma Directory: /home/himanshu/dev/output/gemma3_rag_concise
2025-11-30 14:32:45 - INFO - OpenAI Directory: /home/himanshu/dev/test/data/processed/qa_pairs_individual_components_comprehensive
2025-11-30 14:32:45 - INFO - ======================================================================
2025-11-30 14:32:45 - INFO - Loading 178 Qwen answer files...
2025-11-30 14:32:45 - INFO - Loaded 178 Qwen files
2025-11-30 14:32:45 - INFO - Loading 178 Gemma answer files...
2025-11-30 14:32:45 - INFO - Loaded 178 Gemma files
2025-11-30 14:32:45 - INFO - Loading 178 OpenAI answer files...
2025-11-30 14:32:45 - INFO - Loaded 178 OpenAI files
2025-11-30 14:32:45 - INFO - Matching QA pairs...
2025-11-30 14:32:45 - INFO - Matched 712 question-answer triplets
2025-11-30 14:32:45 - INFO - Calculating ROUGE scores...
2025-11-30 14:32:45 - INFO - Initializing ROUGE scorer...
2025-11-30 14:32:45 - INFO - Using default tokenizer.
2025-11-30 14:32:46 - INFO - Processed 100/712 pairs...
2025-11-30 14:32:47 - INFO - Processed 200/712 pairs...
2025-11-30 14:32:48 - INFO - Processed 300/712 pairs...
2025-11-30 14:32:49 - INFO - Processed 400/712 pairs...
2025-11-30 14:32:50 - INFO - Processed 500/712 pairs...
2025-11-30 14:32:51 - INFO - Processed 600/712 pairs...
2025-11-30 14:32:52 - INFO - Processed 700/712 pairs...
2025-11-30 14:32:52 - INFO - Completed ROUGE calculation for 712 pairs
2025-11-30 14:32:52 - INFO - Calculating aggregated statistics...
2025-11-30 14:32:52 - INFO - Saving results...
2025-11-30 14:32:52 - INFO - Saved detailed results to: /home/himanshu/dev/code/evaluation/results/per_question_rouge_qwen_gemma.csv
2025-11-30 14:32:52 - INFO - Saved summary metrics to: /home/himanshu/dev/code/evaluation/results/summary_rouge_qwen_gemma.json
2025-11-30 14:32:52 - INFO - Saved comparison metrics to: /home/himanshu/dev/code/evaluation/results/comparison_rouge_qwen_gemma.json
2025-11-30 14:32:52 - INFO - ROUGE score calculation complete!
2025-11-30 14:32:52 - INFO - Results saved to: /home/himanshu/dev/code/evaluation/results
2025-11-30 14:32:52 - INFO - Files: per_question_rouge_qwen_gemma.csv, summary_rouge_qwen_gemma.json, comparison_rouge_qwen_gemma.json

======================================================================
ROUGE Score Summary - Qwen RAG Concise vs Gemma RAG Concise
======================================================================
Qwen: RAG Concise (character limits: Q1=600, Q2=1000, Q3=1800, Q4=2000)
Gemma: RAG Concise (character limits: Q1=600, Q2=1000, Q3=1800, Q4=2000)
OpenAI: Baseline (comprehensive)
======================================================================

Total Question-Answer Pairs: 712

----------------------------------------------------------------------
QWEN Overall Corpus ROUGE Scores (F-measure):
----------------------------------------------------------------------
  ROUGE1:
    Mean:   0.4280
    Median: 0.3891
    Std:    0.1805
    Range:  [0.0000, 1.0000]
  ROUGE2:
    Mean:   0.2025
    Median: 0.1522
    Std:    0.1698
    Range:  [0.0000, 1.0000]
  ROUGEL:
    Mean:   0.3392
    Median: 0.2778
    Std:    0.1885
    Range:  [0.0000, 1.0000]
  ROUGELSUM:
    Mean:   0.3494
    Median: 0.2917
    Std:    0.1843
    Range:  [0.0000, 1.0000]

----------------------------------------------------------------------
GEMMA Overall Corpus ROUGE Scores (F-measure):
----------------------------------------------------------------------
  ROUGE1:
    Mean:   0.2953
    Median: 0.2811
    Std:    0.1353
    Range:  [0.0476, 0.9375]
  ROUGE2:
    Mean:   0.1082
    Median: 0.0849
    Std:    0.1009
    Range:  [0.0000, 0.8667]
  ROUGEL:
    Mean:   0.2159
    Median: 0.1971
    Std:    0.1165
    Range:  [0.0397, 0.9375]
  ROUGELSUM:
    Mean:   0.2265
    Median: 0.2102
    Std:    0.1189
    Range:  [0.0397, 0.9375]

----------------------------------------------------------------------
COMPARISON: Qwen vs Gemma (F-measure)
----------------------------------------------------------------------
  ROUGE1:
    Qwen Mean:  0.4280
    Gemma Mean: 0.2953
    Difference: 0.1327 (+13.27%)
    Winner:     Qwen
  ROUGE2:
    Qwen Mean:  0.2025
    Gemma Mean: 0.1082
    Difference: 0.0943 (+9.43%)
    Winner:     Qwen
  ROUGEL:
    Qwen Mean:  0.3392
    Gemma Mean: 0.2159
    Difference: 0.1233 (+12.33%)
    Winner:     Qwen
  ROUGELSUM:
    Qwen Mean:  0.3494
    Gemma Mean: 0.2265
    Difference: 0.1229 (+12.29%)
    Winner:     Qwen

----------------------------------------------------------------------
Per-Question-Type Comparison (ROUGE-1 F-measure):
----------------------------------------------------------------------
  Q1:
    Qwen:  0.3444 (median: 0.3423)
    Gemma: 0.3295 (median: 0.3393)
    Diff:  0.0149 (+1.49%)
    Winner: Qwen
  Q2:
    Qwen:  0.6303 (median: 0.6452)
    Gemma: 0.3532 (median: 0.3246)
    Diff:  0.2770 (+27.70%)
    Winner: Qwen
  Q3:
    Qwen:  0.3915 (median: 0.3846)
    Gemma: 0.2784 (median: 0.2623)
    Diff:  0.1130 (+11.30%)
    Winner: Qwen
  Q4:
    Qwen:  0.3460 (median: 0.3459)
    Gemma: 0.2202 (median: 0.2110)
    Diff:  0.1258 (+12.58%)
    Winner: Qwen

----------------------------------------------------------------------
Answer Length Statistics:
----------------------------------------------------------------------
  QWEN:
    Mean:   218.7 chars
    Median: 200.0 chars
    Range:  [11, 697] chars
  GEMMA:
    Mean:   231.4 chars
    Median: 214.0 chars
    Range:  [47, 640] chars
  OPENAI:
    Mean:   532.5 chars
    Median: 516.0 chars
    Range:  [80, 1753] chars

======================================================================
2025-11-30 14:32:52 - INFO - ✅ ROUGE Score Calculator completed successfully
2025-11-30 14:32:52 - INFO - ======================================================================
2025-11-30 14:32:52 - INFO - Running BERTScore Calculator...
2025-11-30 14:32:52 - INFO - ======================================================================
/home/himanshu/dev/code/.venv_phi4_req/lib64/python3.11/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2025-11-30 14:32:58 - INFO - ======================================================================
2025-11-30 14:32:58 - INFO - BERTScore Calculator: Qwen RAG Concise vs Gemma RAG Concise
2025-11-30 14:32:58 - INFO - ======================================================================
2025-11-30 14:32:58 - INFO - Qwen Directory: /home/himanshu/dev/output/qwen_rag_concise
2025-11-30 14:32:58 - INFO - Gemma Directory: /home/himanshu/dev/output/gemma3_rag_concise
2025-11-30 14:32:58 - INFO - OpenAI Directory: /home/himanshu/dev/test/data/processed/qa_pairs_individual_components_comprehensive
2025-11-30 14:32:58 - INFO - Model: microsoft/deberta-xlarge-mnli
2025-11-30 14:32:58 - INFO - Device: cuda
2025-11-30 14:32:58 - INFO - Batch size: 32
2025-11-30 14:32:58 - INFO - ======================================================================
2025-11-30 14:32:58 - INFO - Loading 178 Qwen answer files...
2025-11-30 14:32:58 - INFO - Loaded 178 Qwen files
2025-11-30 14:32:58 - INFO - Loading 178 Gemma answer files...
2025-11-30 14:32:58 - INFO - Loaded 178 Gemma files
2025-11-30 14:32:58 - INFO - Loading 178 OpenAI answer files...
2025-11-30 14:32:58 - INFO - Loaded 178 OpenAI files
2025-11-30 14:32:58 - INFO - Matching QA pairs...
2025-11-30 14:32:58 - INFO - Matched 712 question-answer triplets
2025-11-30 14:32:58 - INFO - Calculating BERTScore using model: microsoft/deberta-xlarge-mnli
2025-11-30 14:32:58 - INFO - Device: cuda
2025-11-30 14:32:58 - INFO - Processing 712 pairs in batches of 32...
2025-11-30 14:32:58 - INFO - Calculating BERTScore for Qwen vs OpenAI...
2025-11-30 14:32:58 - INFO - Loading BERT model (this may take a moment on first run)...
calculating scores...
computing bert embedding.
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:03<02:20,  3.18s/it]  4%|▍         | 2/45 [00:06<02:08,  2.99s/it]  7%|▋         | 3/45 [00:08<02:03,  2.94s/it]  9%|▉         | 4/45 [00:10<01:44,  2.54s/it] 11%|█         | 5/45 [00:12<01:34,  2.37s/it] 13%|█▎        | 6/45 [00:14<01:22,  2.12s/it] 16%|█▌        | 7/45 [00:15<01:07,  1.79s/it] 18%|█▊        | 8/45 [00:16<00:57,  1.55s/it] 20%|██        | 9/45 [00:17<00:51,  1.43s/it] 22%|██▏       | 10/45 [00:18<00:45,  1.31s/it] 24%|██▍       | 11/45 [00:20<00:43,  1.28s/it] 27%|██▋       | 12/45 [00:21<00:41,  1.27s/it] 29%|██▉       | 13/45 [00:22<00:37,  1.17s/it] 31%|███       | 14/45 [00:23<00:34,  1.10s/it] 33%|███▎      | 15/45 [00:24<00:30,  1.03s/it] 36%|███▌      | 16/45 [00:25<00:29,  1.00s/it] 38%|███▊      | 17/45 [00:25<00:26,  1.07it/s] 40%|████      | 18/45 [00:26<00:25,  1.07it/s] 42%|████▏     | 19/45 [00:27<00:21,  1.23it/s] 44%|████▍     | 20/45 [00:27<00:18,  1.37it/s] 47%|████▋     | 21/45 [00:28<00:15,  1.53it/s] 49%|████▉     | 22/45 [00:28<00:14,  1.64it/s] 51%|█████     | 23/45 [00:29<00:12,  1.77it/s] 53%|█████▎    | 24/45 [00:29<00:11,  1.89it/s] 56%|█████▌    | 25/45 [00:30<00:10,  1.99it/s] 58%|█████▊    | 26/45 [00:30<00:09,  2.10it/s] 60%|██████    | 27/45 [00:30<00:08,  2.18it/s] 62%|██████▏   | 28/45 [00:31<00:07,  2.27it/s] 64%|██████▍   | 29/45 [00:31<00:06,  2.32it/s] 67%|██████▋   | 30/45 [00:32<00:05,  2.57it/s] 69%|██████▉   | 31/45 [00:32<00:05,  2.62it/s] 71%|███████   | 32/45 [00:32<00:04,  2.92it/s] 73%|███████▎  | 33/45 [00:32<00:03,  3.08it/s] 76%|███████▌  | 34/45 [00:33<00:03,  3.31it/s] 78%|███████▊  | 35/45 [00:33<00:02,  3.42it/s] 80%|████████  | 36/45 [00:33<00:02,  3.56it/s] 82%|████████▏ | 37/45 [00:34<00:02,  3.59it/s] 84%|████████▍ | 38/45 [00:34<00:01,  3.89it/s] 87%|████████▋ | 39/45 [00:34<00:01,  4.05it/s] 89%|████████▉ | 40/45 [00:34<00:01,  3.93it/s] 91%|█████████ | 41/45 [00:34<00:01,  3.92it/s] 93%|█████████▎| 42/45 [00:35<00:00,  4.16it/s] 96%|█████████▌| 43/45 [00:36<00:00,  2.07it/s] 98%|█████████▊| 44/45 [00:36<00:00,  2.38it/s]100%|██████████| 45/45 [00:36<00:00,  3.08it/s]100%|██████████| 45/45 [00:36<00:00,  1.23it/s]
computing greedy matching.
  0%|          | 0/23 [00:00<?, ?it/s] 22%|██▏       | 5/23 [00:00<00:00, 47.39it/s] 57%|█████▋    | 13/23 [00:00<00:00, 64.69it/s] 91%|█████████▏| 21/23 [00:00<00:00, 70.01it/s]100%|██████████| 23/23 [00:00<00:00, 69.50it/s]
2025-11-30 14:33:40 - INFO - Qwen BERTScore calculation complete!
2025-11-30 14:33:40 - INFO - Calculating BERTScore for Gemma vs OpenAI...
done in 36.97 seconds, 19.26 sentences/sec
calculating scores...
computing bert embedding.
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:03<02:12,  3.02s/it]  4%|▍         | 2/45 [00:05<02:05,  2.92s/it]  7%|▋         | 3/45 [00:08<02:01,  2.90s/it]  9%|▉         | 4/45 [00:10<01:39,  2.42s/it] 11%|█         | 5/45 [00:12<01:31,  2.30s/it] 13%|█▎        | 6/45 [00:14<01:21,  2.08s/it] 16%|█▌        | 7/45 [00:15<01:07,  1.77s/it] 18%|█▊        | 8/45 [00:16<00:58,  1.59s/it] 20%|██        | 9/45 [00:17<00:52,  1.46s/it] 22%|██▏       | 10/45 [00:18<00:46,  1.34s/it] 24%|██▍       | 11/45 [00:19<00:44,  1.30s/it] 27%|██▋       | 12/45 [00:21<00:40,  1.24s/it] 29%|██▉       | 13/45 [00:22<00:39,  1.22s/it] 31%|███       | 14/45 [00:23<00:36,  1.17s/it] 33%|███▎      | 15/45 [00:24<00:33,  1.10s/it] 36%|███▌      | 16/45 [00:25<00:31,  1.08s/it] 38%|███▊      | 17/45 [00:26<00:29,  1.04s/it] 40%|████      | 18/45 [00:26<00:24,  1.11it/s] 42%|████▏     | 19/45 [00:27<00:22,  1.14it/s] 44%|████▍     | 20/45 [00:28<00:20,  1.25it/s] 47%|████▋     | 21/45 [00:29<00:20,  1.19it/s] 49%|████▉     | 22/45 [00:29<00:16,  1.38it/s] 51%|█████     | 23/45 [00:30<00:15,  1.44it/s] 53%|█████▎    | 24/45 [00:30<00:13,  1.61it/s] 56%|█████▌    | 25/45 [00:31<00:11,  1.76it/s] 58%|█████▊    | 26/45 [00:31<00:09,  1.95it/s] 60%|██████    | 27/45 [00:31<00:08,  2.02it/s] 62%|██████▏   | 28/45 [00:32<00:07,  2.24it/s] 64%|██████▍   | 29/45 [00:32<00:07,  2.17it/s] 67%|██████▋   | 30/45 [00:33<00:06,  2.16it/s] 69%|██████▉   | 31/45 [00:33<00:06,  2.19it/s] 71%|███████   | 32/45 [00:34<00:05,  2.40it/s] 73%|███████▎  | 33/45 [00:34<00:04,  2.58it/s] 76%|███████▌  | 34/45 [00:34<00:04,  2.67it/s] 78%|███████▊  | 35/45 [00:35<00:03,  2.75it/s] 80%|████████  | 36/45 [00:35<00:03,  2.90it/s] 82%|████████▏ | 37/45 [00:35<00:02,  3.07it/s] 84%|████████▍ | 38/45 [00:35<00:02,  3.27it/s] 87%|████████▋ | 39/45 [00:36<00:01,  3.33it/s] 89%|████████▉ | 40/45 [00:36<00:01,  3.39it/s] 91%|█████████ | 41/45 [00:36<00:01,  3.63it/s] 93%|█████████▎| 42/45 [00:36<00:00,  3.77it/s] 96%|█████████▌| 43/45 [00:37<00:00,  3.70it/s] 98%|█████████▊| 44/45 [00:37<00:00,  3.96it/s]100%|██████████| 45/45 [00:37<00:00,  1.20it/s]
computing greedy matching.
  0%|          | 0/23 [00:00<?, ?it/s] 35%|███▍      | 8/23 [00:00<00:00, 71.26it/s] 70%|██████▉   | 16/23 [00:00<00:00, 73.53it/s]100%|██████████| 23/23 [00:00<00:00, 76.32it/s]
2025-11-30 14:34:22 - INFO - Gemma BERTScore calculation complete!
2025-11-30 14:34:22 - INFO - Calculating aggregated statistics...
2025-11-30 14:34:22 - INFO - Saving results...
2025-11-30 14:34:22 - INFO - Saved detailed results to: /home/himanshu/dev/code/evaluation/results/per_question_bertscore_qwen_gemma.csv
2025-11-30 14:34:22 - INFO - Saved summary metrics to: /home/himanshu/dev/code/evaluation/results/summary_bertscore_qwen_gemma.json
2025-11-30 14:34:22 - INFO - Saved comparison metrics to: /home/himanshu/dev/code/evaluation/results/comparison_bertscore_qwen_gemma.json
2025-11-30 14:34:22 - INFO - BERTScore calculation complete!
2025-11-30 14:34:22 - INFO - Results saved to: /home/himanshu/dev/code/evaluation/results
2025-11-30 14:34:22 - INFO - Files: per_question_bertscore_qwen_gemma.csv, summary_bertscore_qwen_gemma.json, comparison_bertscore_qwen_gemma.json
done in 37.82 seconds, 18.83 sentences/sec

======================================================================
BERTScore Summary - Qwen RAG Concise vs Gemma RAG Concise
======================================================================
Qwen: RAG Concise (character limits: Q1=600, Q2=1000, Q3=1800, Q4=2000)
Gemma: RAG Concise (character limits: Q1=600, Q2=1000, Q3=1800, Q4=2000)
OpenAI: Baseline (comprehensive)
======================================================================

Total Question-Answer Pairs: 712

----------------------------------------------------------------------
QWEN Overall Corpus BERTScore:
----------------------------------------------------------------------
  PRECISION:
    Mean:   0.5012
    Median: 0.4577
    Std:    0.1969
    Range:  [-0.3919, 1.0000]
  RECALL:
    Mean:   0.3084
    Median: 0.2344
    Std:    0.2496
    Range:  [-0.4533, 1.0000]
  F1:
    Mean:   0.3973
    Median: 0.3382
    Std:    0.2185
    Range:  [-0.2882, 1.0000]

----------------------------------------------------------------------
GEMMA Overall Corpus BERTScore:
----------------------------------------------------------------------
  PRECISION:
    Mean:   0.2438
    Median: 0.1836
    Std:    0.1904
    Range:  [-0.1145, 0.9164]
  RECALL:
    Mean:   0.1143
    Median: 0.0898
    Std:    0.2025
    Range:  [-0.4884, 0.9358]
  F1:
    Mean:   0.1751
    Median: 0.1448
    Std:    0.1834
    Range:  [-0.3255, 0.9100]

----------------------------------------------------------------------
COMPARISON: Qwen vs Gemma
----------------------------------------------------------------------
  F1 Score:
    Qwen Mean:  0.3973
    Gemma Mean: 0.1751
    Difference: 0.2222 (+22.22%)
    Winner:     Qwen

  Precision:
    Qwen Mean:  0.5012
    Gemma Mean: 0.2438
    Difference: 0.2574 (+25.74%)
    Winner:     Qwen

  Recall:
    Qwen Mean:  0.3084
    Gemma Mean: 0.1143
    Difference: 0.1942 (+19.42%)
    Winner:     Qwen

----------------------------------------------------------------------
Per-Question-Type Comparison (F1):
----------------------------------------------------------------------
  Q1:
    Qwen:  0.3201 (median: 0.3172)
    Gemma: 0.2869 (median: 0.2925)
    Diff:  0.0332 (+3.32%)
    Winner: Qwen
  Q2:
    Qwen:  0.6692 (median: 0.7207)
    Gemma: 0.1903 (median: 0.1436)
    Diff:  0.4790 (+47.90%)
    Winner: Qwen
  Q3:
    Qwen:  0.3212 (median: 0.2952)
    Gemma: 0.1350 (median: 0.0802)
    Diff:  0.1863 (+18.63%)
    Winner: Qwen
  Q4:
    Qwen:  0.2788 (median: 0.2736)
    Gemma: 0.0884 (median: 0.0556)
    Diff:  0.1904 (+19.04%)
    Winner: Qwen

----------------------------------------------------------------------
Answer Length Statistics:
----------------------------------------------------------------------
  QWEN:
    Mean:   218.7 chars
    Median: 200.0 chars
    Range:  [11, 697] chars
  GEMMA:
    Mean:   231.4 chars
    Median: 214.0 chars
    Range:  [47, 640] chars
  OPENAI:
    Mean:   532.5 chars
    Median: 516.0 chars
    Range:  [80, 1753] chars

======================================================================
2025-11-30 14:34:23 - INFO - ✅ BERTScore Calculator completed successfully
2025-11-30 14:34:23 - INFO - ======================================================================
2025-11-30 14:34:23 - INFO - Evaluation complete at: 2025-11-30 14:34:23
2025-11-30 14:34:23 - INFO - ✅ All evaluations completed successfully!
2025-11-30 14:34:23 - INFO - ======================================================================

======================================================================
EVALUATION SUMMARY
======================================================================

Execution Times:
  BLEU: 4.58 seconds
  ROUGE: 8.45 seconds
  BERTSCORE: 90.65 seconds

Result Files Generated:

  BLEU:
    CSV:        ✅
    Summary:    ✅
    Comparison: ✅

  ROUGE:
    CSV:        ✅
    Summary:    ✅
    Comparison: ✅

  BERTSCORE:
    CSV:        ✅
    Summary:    ✅
    Comparison: ✅

======================================================================
All results saved to: /home/himanshu/dev/code/evaluation/results
======================================================================

======================================================================
QUICK METRICS SUMMARY
======================================================================

BLEU-4 Scores:
  Qwen:  0.0906
  Gemma: 0.0427
  Winner: Qwen

ROUGE-1 F-measure:
  Qwen:  0.4280
  Gemma: 0.2953
  Winner: Qwen

BERTScore F1:
  Qwen:  0.3973
  Gemma: 0.1751
  Winner: Qwen

======================================================================
