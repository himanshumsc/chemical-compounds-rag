2025-11-30 10:47:34 - INFO - Using virtual environment Python: /home/himanshu/dev/code/.venv_phi4_req/bin/python3
2025-11-30 10:47:34 - INFO - ======================================================================
2025-11-30 10:47:34 - INFO - Qwen vs Gemma RAG Concise - Complete Evaluation
2025-11-30 10:47:34 - INFO - ======================================================================
2025-11-30 10:47:34 - INFO - Start time: 2025-11-30 10:47:34
2025-11-30 10:47:34 - INFO - Results directory: results
2025-11-30 10:47:34 - INFO - Using Python: /home/himanshu/dev/code/.venv_phi4_req/bin/python3
2025-11-30 10:47:34 - INFO - ======================================================================
2025-11-30 10:47:34 - INFO - ======================================================================
2025-11-30 10:47:34 - INFO - Running BLEU Score Calculator...
2025-11-30 10:47:34 - INFO - ======================================================================
2025-11-30 10:47:35 - INFO - ======================================================================
2025-11-30 10:47:35 - INFO - BLEU Score Calculator: Qwen RAG Concise vs Gemma RAG Concise
2025-11-30 10:47:35 - INFO - ======================================================================
2025-11-30 10:47:35 - INFO - Qwen Directory: /home/himanshu/dev/output/qwen_rag_concise
2025-11-30 10:47:35 - INFO - Gemma Directory: /home/himanshu/dev/output/gemma3_rag_concise
2025-11-30 10:47:35 - INFO - OpenAI Directory: /home/himanshu/dev/test/data/processed/qa_pairs_individual_components_comprehensive
2025-11-30 10:47:35 - INFO - ======================================================================
2025-11-30 10:47:35 - INFO - Loading 178 Qwen answer files...
2025-11-30 10:47:35 - INFO - Loaded 178 Qwen files
2025-11-30 10:47:35 - INFO - Loading 178 Gemma answer files...
2025-11-30 10:47:35 - INFO - Loaded 178 Gemma files
2025-11-30 10:47:35 - INFO - Loading 178 OpenAI answer files...
2025-11-30 10:47:35 - INFO - Loaded 178 OpenAI files
2025-11-30 10:47:35 - INFO - Matching QA pairs...
2025-11-30 10:47:35 - INFO - Matched 712 question-answer triplets
2025-11-30 10:47:35 - INFO - Calculating BLEU scores...
2025-11-30 10:47:35 - INFO - Processed 100/712 pairs...
2025-11-30 10:47:36 - INFO - Processed 200/712 pairs...
2025-11-30 10:47:36 - INFO - Processed 300/712 pairs...
2025-11-30 10:47:37 - INFO - Processed 400/712 pairs...
2025-11-30 10:47:37 - INFO - Processed 500/712 pairs...
2025-11-30 10:47:37 - INFO - Processed 600/712 pairs...
2025-11-30 10:47:38 - INFO - Processed 700/712 pairs...
2025-11-30 10:47:38 - INFO - Completed BLEU calculation for 712 pairs
2025-11-30 10:47:38 - INFO - Calculating aggregated statistics...
2025-11-30 10:47:38 - INFO - Saving results...
2025-11-30 10:47:38 - INFO - Saved detailed results to: /home/himanshu/dev/code/evaluation/results/per_question_bleu_qwen_gemma.csv
2025-11-30 10:47:38 - INFO - Saved summary metrics to: /home/himanshu/dev/code/evaluation/results/summary_metrics_qwen_gemma.json
2025-11-30 10:47:38 - INFO - Saved comparison metrics to: /home/himanshu/dev/code/evaluation/results/comparison_bleu_qwen_gemma.json
2025-11-30 10:47:38 - INFO - BLEU score calculation complete!
2025-11-30 10:47:38 - INFO - Results saved to: /home/himanshu/dev/code/evaluation/results
2025-11-30 10:47:38 - INFO - Files: per_question_bleu_qwen_gemma.csv, summary_metrics_qwen_gemma.json, comparison_bleu_qwen_gemma.json

======================================================================
BLEU Score Summary - Qwen RAG Concise vs Gemma RAG Concise
======================================================================
Qwen: RAG Concise (character limits: Q1=600, Q2=1000, Q3=1800, Q4=2000)
Gemma: RAG Concise (character limits: Q1=600, Q2=1000, Q3=1800, Q4=2000)
OpenAI: Baseline (comprehensive)
======================================================================

Total Question-Answer Pairs: 712

----------------------------------------------------------------------
QWEN Overall Corpus BLEU Scores:
----------------------------------------------------------------------
  BLEU_1:
    Mean:   0.2469
    Median: 0.1867
    Std:    0.2210
    Range:  [0.0000, 1.0000]
  BLEU_2:
    Mean:   0.1741
    Median: 0.1104
    Std:    0.1893
    Range:  [0.0000, 1.0000]
  BLEU_3:
    Mean:   0.1235
    Median: 0.0622
    Std:    0.1677
    Range:  [0.0000, 1.0000]
  BLEU_4:
    Mean:   0.0914
    Median: 0.0353
    Std:    0.1525
    Range:  [0.0000, 1.0000]

----------------------------------------------------------------------
GEMMA Overall Corpus BLEU Scores:
----------------------------------------------------------------------
  BLEU_1:
    Mean:   0.1892
    Median: 0.1625
    Std:    0.1441
    Range:  [0.0000, 0.8527]
  BLEU_2:
    Mean:   0.1089
    Median: 0.0809
    Std:    0.1047
    Range:  [0.0000, 0.7726]
  BLEU_3:
    Mean:   0.0640
    Median: 0.0383
    Std:    0.0817
    Range:  [0.0000, 0.6831]
  BLEU_4:
    Mean:   0.0411
    Median: 0.0203
    Std:    0.0658
    Range:  [0.0000, 0.6141]

----------------------------------------------------------------------
COMPARISON: Qwen vs Gemma (BLEU-4)
----------------------------------------------------------------------
  Qwen Mean BLEU-4:  0.0914
  Gemma Mean BLEU-4: 0.0411
  Difference:        0.0503 (+5.03%)
  Winner:            Qwen

----------------------------------------------------------------------
Per-Question-Type Comparison (BLEU-4):
----------------------------------------------------------------------
  Q1:
    Qwen:  0.0317 (median: 0.0194)
    Gemma: 0.0300 (median: 0.0186)
    Diff:  0.0017 (+0.17%)
    Winner: Qwen
  Q2:
    Qwen:  0.2168 (median: 0.0974)
    Gemma: 0.0707 (median: 0.0290)
    Diff:  0.1461 (+14.61%)
    Winner: Qwen
  Q3:
    Qwen:  0.0759 (median: 0.0494)
    Gemma: 0.0449 (median: 0.0181)
    Diff:  0.0311 (+3.11%)
    Winner: Qwen
  Q4:
    Qwen:  0.0413 (median: 0.0252)
    Gemma: 0.0188 (median: 0.0080)
    Diff:  0.0225 (+2.25%)
    Winner: Qwen

----------------------------------------------------------------------
Answer Length Statistics:
----------------------------------------------------------------------
  QWEN:
    Mean:   218.7 chars
    Median: 200.0 chars
    Range:  [11, 697] chars
  GEMMA:
    Mean:   231.4 chars
    Median: 214.0 chars
    Range:  [47, 640] chars
  OPENAI:
    Mean:   522.2 chars
    Median: 510.5 chars
    Range:  [80, 2251] chars

======================================================================
2025-11-30 10:47:38 - INFO - ✅ BLEU Score Calculator completed successfully
2025-11-30 10:47:38 - INFO - ======================================================================
2025-11-30 10:47:38 - INFO - Running ROUGE Score Calculator...
2025-11-30 10:47:38 - INFO - ======================================================================
Error: rouge-score not installed. Install with: pip install rouge-score
2025-11-30 10:47:38 - ERROR - ❌ ROUGE Score Calculator failed with exit code 1
2025-11-30 10:47:38 - WARNING - ROUGE evaluation failed, continuing with other metrics...
2025-11-30 10:47:38 - INFO - ======================================================================
2025-11-30 10:47:38 - INFO - Running BERTScore Calculator...
2025-11-30 10:47:38 - INFO - ======================================================================
/home/himanshu/dev/code/.venv_phi4_req/lib64/python3.11/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2025-11-30 10:47:43 - INFO - ======================================================================
2025-11-30 10:47:43 - INFO - BERTScore Calculator: Qwen RAG Concise vs Gemma RAG Concise
2025-11-30 10:47:43 - INFO - ======================================================================
2025-11-30 10:47:43 - INFO - Qwen Directory: /home/himanshu/dev/output/qwen_rag_concise
2025-11-30 10:47:43 - INFO - Gemma Directory: /home/himanshu/dev/output/gemma3_rag_concise
2025-11-30 10:47:43 - INFO - OpenAI Directory: /home/himanshu/dev/test/data/processed/qa_pairs_individual_components_comprehensive
2025-11-30 10:47:43 - INFO - Model: microsoft/deberta-xlarge-mnli
2025-11-30 10:47:43 - INFO - Device: cuda
2025-11-30 10:47:43 - INFO - Batch size: 32
2025-11-30 10:47:43 - INFO - ======================================================================
2025-11-30 10:47:43 - INFO - Loading 178 Qwen answer files...
2025-11-30 10:47:43 - INFO - Loaded 178 Qwen files
2025-11-30 10:47:43 - INFO - Loading 178 Gemma answer files...
2025-11-30 10:47:43 - INFO - Loaded 178 Gemma files
2025-11-30 10:47:43 - INFO - Loading 178 OpenAI answer files...
2025-11-30 10:47:43 - INFO - Loaded 178 OpenAI files
2025-11-30 10:47:43 - INFO - Matching QA pairs...
2025-11-30 10:47:43 - INFO - Matched 712 question-answer triplets
2025-11-30 10:47:43 - INFO - Calculating BERTScore using model: microsoft/deberta-xlarge-mnli
2025-11-30 10:47:43 - INFO - Device: cuda
2025-11-30 10:47:43 - INFO - Processing 712 pairs in batches of 32...
2025-11-30 10:47:43 - INFO - Calculating BERTScore for Qwen vs OpenAI...
2025-11-30 10:47:43 - INFO - Loading BERT model (this may take a moment on first run)...
calculating scores...
computing bert embedding.
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:05<03:50,  5.25s/it]  4%|▍         | 2/45 [00:07<02:41,  3.75s/it]  7%|▋         | 3/45 [00:10<02:16,  3.25s/it]  9%|▉         | 4/45 [00:12<01:59,  2.91s/it] 11%|█         | 5/45 [00:14<01:34,  2.37s/it] 13%|█▎        | 6/45 [00:15<01:16,  1.97s/it] 16%|█▌        | 7/45 [00:16<01:04,  1.70s/it] 18%|█▊        | 8/45 [00:17<00:57,  1.56s/it] 20%|██        | 9/45 [00:19<00:51,  1.43s/it] 22%|██▏       | 10/45 [00:20<00:45,  1.31s/it] 24%|██▍       | 11/45 [00:21<00:43,  1.28s/it] 27%|██▋       | 12/45 [00:22<00:41,  1.27s/it] 29%|██▉       | 13/45 [00:23<00:37,  1.17s/it] 31%|███       | 14/45 [00:24<00:35,  1.13s/it] 33%|███▎      | 15/45 [00:25<00:31,  1.05s/it] 36%|███▌      | 16/45 [00:26<00:27,  1.06it/s] 38%|███▊      | 17/45 [00:26<00:24,  1.13it/s] 40%|████      | 18/45 [00:27<00:21,  1.24it/s] 42%|████▏     | 19/45 [00:28<00:18,  1.39it/s] 44%|████▍     | 20/45 [00:28<00:16,  1.53it/s] 47%|████▋     | 21/45 [00:29<00:15,  1.57it/s] 49%|████▉     | 22/45 [00:29<00:13,  1.69it/s] 51%|█████     | 23/45 [00:30<00:12,  1.83it/s] 53%|█████▎    | 24/45 [00:30<00:10,  1.98it/s] 56%|█████▌    | 25/45 [00:30<00:09,  2.08it/s] 58%|█████▊    | 26/45 [00:31<00:08,  2.15it/s] 60%|██████    | 27/45 [00:31<00:08,  2.24it/s] 62%|██████▏   | 28/45 [00:32<00:07,  2.32it/s] 64%|██████▍   | 29/45 [00:32<00:06,  2.58it/s] 67%|██████▋   | 30/45 [00:32<00:05,  2.62it/s] 69%|██████▉   | 31/45 [00:33<00:05,  2.66it/s] 71%|███████   | 32/45 [00:33<00:04,  2.68it/s] 73%|███████▎  | 33/45 [00:33<00:04,  2.97it/s] 76%|███████▌  | 34/45 [00:34<00:03,  3.15it/s] 78%|███████▊  | 35/45 [00:34<00:02,  3.36it/s] 80%|████████  | 36/45 [00:34<00:02,  3.43it/s] 82%|████████▏ | 37/45 [00:34<00:02,  3.69it/s] 84%|████████▍ | 38/45 [00:35<00:01,  3.71it/s] 87%|████████▋ | 39/45 [00:35<00:01,  3.91it/s] 89%|████████▉ | 40/45 [00:35<00:01,  3.84it/s] 91%|█████████ | 41/45 [00:35<00:01,  3.89it/s] 93%|█████████▎| 42/45 [00:36<00:00,  4.23it/s] 96%|█████████▌| 43/45 [00:37<00:00,  2.08it/s] 98%|█████████▊| 44/45 [00:37<00:00,  2.39it/s]100%|██████████| 45/45 [00:37<00:00,  1.20it/s]
computing greedy matching.
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:00<00:06,  3.39it/s] 39%|███▉      | 9/23 [00:00<00:00, 27.99it/s] 74%|███████▍  | 17/23 [00:00<00:00, 43.76it/s]100%|██████████| 23/23 [00:00<00:00, 40.14it/s]
2025-11-30 10:48:27 - INFO - Qwen BERTScore calculation complete!
2025-11-30 10:48:27 - INFO - Calculating BERTScore for Gemma vs OpenAI...
done in 38.04 seconds, 18.72 sentences/sec
calculating scores...
computing bert embedding.
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:04<03:26,  4.69s/it]  4%|▍         | 2/45 [00:07<02:30,  3.51s/it]  7%|▋         | 3/45 [00:10<02:10,  3.11s/it]  9%|▉         | 4/45 [00:12<01:52,  2.74s/it] 11%|█         | 5/45 [00:13<01:30,  2.27s/it] 13%|█▎        | 6/45 [00:14<01:14,  1.91s/it] 16%|█▌        | 7/45 [00:15<01:03,  1.67s/it] 18%|█▊        | 8/45 [00:17<00:56,  1.54s/it] 20%|██        | 9/45 [00:18<00:51,  1.42s/it] 22%|██▏       | 10/45 [00:19<00:47,  1.36s/it] 24%|██▍       | 11/45 [00:20<00:43,  1.27s/it] 27%|██▋       | 12/45 [00:21<00:39,  1.21s/it] 29%|██▉       | 13/45 [00:22<00:38,  1.20s/it] 31%|███       | 14/45 [00:24<00:35,  1.16s/it] 33%|███▎      | 15/45 [00:24<00:32,  1.08s/it] 36%|███▌      | 16/45 [00:25<00:30,  1.06s/it] 38%|███▊      | 17/45 [00:26<00:26,  1.04it/s] 40%|████      | 18/45 [00:27<00:24,  1.09it/s] 42%|████▏     | 19/45 [00:28<00:21,  1.23it/s] 44%|████▍     | 20/45 [00:28<00:21,  1.18it/s] 47%|████▋     | 21/45 [00:29<00:17,  1.34it/s] 49%|████▉     | 22/45 [00:29<00:15,  1.53it/s] 51%|█████     | 23/45 [00:30<00:14,  1.56it/s] 53%|█████▎    | 24/45 [00:30<00:12,  1.71it/s] 56%|█████▌    | 25/45 [00:31<00:10,  1.90it/s] 58%|█████▊    | 26/45 [00:31<00:09,  2.06it/s] 60%|██████    | 27/45 [00:32<00:08,  2.12it/s] 62%|██████▏   | 28/45 [00:32<00:07,  2.26it/s] 64%|██████▍   | 29/45 [00:33<00:07,  2.17it/s] 67%|██████▋   | 30/45 [00:33<00:06,  2.18it/s] 69%|██████▉   | 31/45 [00:33<00:06,  2.18it/s] 71%|███████   | 32/45 [00:34<00:05,  2.43it/s] 73%|███████▎  | 33/45 [00:34<00:04,  2.60it/s] 76%|███████▌  | 34/45 [00:34<00:04,  2.63it/s] 78%|███████▊  | 35/45 [00:35<00:03,  2.81it/s] 80%|████████  | 36/45 [00:35<00:03,  2.99it/s] 82%|████████▏ | 37/45 [00:35<00:02,  3.21it/s] 84%|████████▍ | 38/45 [00:36<00:02,  3.31it/s] 87%|████████▋ | 39/45 [00:36<00:01,  3.35it/s] 89%|████████▉ | 40/45 [00:36<00:01,  3.60it/s] 91%|█████████ | 41/45 [00:36<00:01,  3.59it/s] 93%|█████████▎| 42/45 [00:37<00:00,  3.79it/s] 96%|█████████▌| 43/45 [00:37<00:00,  3.72it/s] 98%|█████████▊| 44/45 [00:37<00:00,  4.14it/s]100%|██████████| 45/45 [00:37<00:00,  1.19it/s]
computing greedy matching.
  0%|          | 0/23 [00:00<?, ?it/s] 35%|███▍      | 8/23 [00:00<00:00, 75.82it/s] 70%|██████▉   | 16/23 [00:00<00:00, 76.74it/s]100%|██████████| 23/23 [00:00<00:00, 79.19it/s]
2025-11-30 10:49:09 - INFO - Gemma BERTScore calculation complete!
2025-11-30 10:49:09 - INFO - Calculating aggregated statistics...
2025-11-30 10:49:09 - INFO - Saving results...
2025-11-30 10:49:09 - INFO - Saved detailed results to: /home/himanshu/dev/code/evaluation/results/per_question_bertscore_qwen_gemma.csv
2025-11-30 10:49:09 - INFO - Saved summary metrics to: /home/himanshu/dev/code/evaluation/results/summary_bertscore_qwen_gemma.json
2025-11-30 10:49:09 - INFO - Saved comparison metrics to: /home/himanshu/dev/code/evaluation/results/comparison_bertscore_qwen_gemma.json
2025-11-30 10:49:09 - INFO - BERTScore calculation complete!
2025-11-30 10:49:09 - INFO - Results saved to: /home/himanshu/dev/code/evaluation/results
2025-11-30 10:49:09 - INFO - Files: per_question_bertscore_qwen_gemma.csv, summary_bertscore_qwen_gemma.json, comparison_bertscore_qwen_gemma.json
done in 37.99 seconds, 18.74 sentences/sec

======================================================================
BERTScore Summary - Qwen RAG Concise vs Gemma RAG Concise
======================================================================
Qwen: RAG Concise (character limits: Q1=600, Q2=1000, Q3=1800, Q4=2000)
Gemma: RAG Concise (character limits: Q1=600, Q2=1000, Q3=1800, Q4=2000)
OpenAI: Baseline (comprehensive)
======================================================================

Total Question-Answer Pairs: 712

----------------------------------------------------------------------
QWEN Overall Corpus BERTScore:
----------------------------------------------------------------------
  PRECISION:
    Mean:   0.5039
    Median: 0.4637
    Std:    0.1984
    Range:  [-0.3919, 1.0000]
  RECALL:
    Mean:   0.3150
    Median: 0.2412
    Std:    0.2499
    Range:  [-0.4533, 1.0000]
  F1:
    Mean:   0.4022
    Median: 0.3398
    Std:    0.2191
    Range:  [-0.2882, 1.0000]

----------------------------------------------------------------------
GEMMA Overall Corpus BERTScore:
----------------------------------------------------------------------
  PRECISION:
    Mean:   0.2429
    Median: 0.1882
    Std:    0.1912
    Range:  [-0.0877, 0.9164]
  RECALL:
    Mean:   0.1169
    Median: 0.0919
    Std:    0.2046
    Range:  [-0.4884, 0.9238]
  F1:
    Mean:   0.1760
    Median: 0.1440
    Std:    0.1843
    Range:  [-0.3255, 0.8935]

----------------------------------------------------------------------
COMPARISON: Qwen vs Gemma
----------------------------------------------------------------------
  F1 Score:
    Qwen Mean:  0.4022
    Gemma Mean: 0.1760
    Difference: 0.2261 (+22.61%)
    Winner:     Qwen

  Precision:
    Qwen Mean:  0.5039
    Gemma Mean: 0.2429
    Difference: 0.2609 (+26.09%)
    Winner:     Qwen

  Recall:
    Qwen Mean:  0.3150
    Gemma Mean: 0.1169
    Difference: 0.1981 (+19.81%)
    Winner:     Qwen

----------------------------------------------------------------------
Per-Question-Type Comparison (F1):
----------------------------------------------------------------------
  Q1:
    Qwen:  0.3201 (median: 0.3172)
    Gemma: 0.2869 (median: 0.2925)
    Diff:  0.0332 (+3.32%)
    Winner: Qwen
  Q2:
    Qwen:  0.6777 (median: 0.7199)
    Gemma: 0.1926 (median: 0.1423)
    Diff:  0.4851 (+48.51%)
    Winner: Qwen
  Q3:
    Qwen:  0.3253 (median: 0.3043)
    Gemma: 0.1345 (median: 0.0843)
    Diff:  0.1909 (+19.09%)
    Winner: Qwen
  Q4:
    Qwen:  0.2856 (median: 0.2858)
    Gemma: 0.0901 (median: 0.0566)
    Diff:  0.1955 (+19.55%)
    Winner: Qwen

----------------------------------------------------------------------
Answer Length Statistics:
----------------------------------------------------------------------
  QWEN:
    Mean:   218.7 chars
    Median: 200.0 chars
    Range:  [11, 697] chars
  GEMMA:
    Mean:   231.4 chars
    Median: 214.0 chars
    Range:  [47, 640] chars
  OPENAI:
    Mean:   522.2 chars
    Median: 510.5 chars
    Range:  [80, 2251] chars

======================================================================
2025-11-30 10:49:11 - INFO - ✅ BERTScore Calculator completed successfully
2025-11-30 10:49:11 - INFO - ======================================================================
2025-11-30 10:49:11 - INFO - Evaluation complete at: 2025-11-30 10:49:11
2025-11-30 10:49:11 - WARNING - ⚠️  Some evaluations failed. Check logs above for details.
2025-11-30 10:49:11 - INFO - ======================================================================

======================================================================
EVALUATION SUMMARY
======================================================================

Execution Times:
  BLEU: 4.44 seconds
  ROUGE: Not executed
  BERTSCORE: 92.41 seconds

Result Files Generated:

  BLEU:
    CSV:        ✅
    Summary:    ✅
    Comparison: ✅

  ROUGE:
    CSV:        ❌
    Summary:    ❌
    Comparison: ❌

  BERTSCORE:
    CSV:        ✅
    Summary:    ✅
    Comparison: ✅

======================================================================
All results saved to: results
======================================================================

======================================================================
QUICK METRICS SUMMARY
======================================================================

BLEU-4 Scores:
  Qwen:  0.0914
  Gemma: 0.0411
  Winner: Qwen

BERTScore F1:
  Qwen:  0.4022
  Gemma: 0.1760
  Winner: Qwen

======================================================================
